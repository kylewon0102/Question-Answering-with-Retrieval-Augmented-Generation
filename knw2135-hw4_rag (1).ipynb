{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cd0e4f49-0952-4e29-8abf-eef049810385",
      "metadata": {
        "id": "cd0e4f49-0952-4e29-8abf-eef049810385"
      },
      "source": [
        "# COMS W4705 - Homework 4\n",
        "## Question Answering with Retrieval Augmented Generation\n",
        "\n",
        "Anubhav Jangra \\<aj3228@columbia.edu\\>, Emile Al-Billeh \\<ea3048@columbia.edu\\>, Daniel Bauer \\<bauer@cs.columbia.edu\\>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0adabf07-f8d4-4e8c-82d5-1a89d3656dd4",
      "metadata": {
        "id": "0adabf07-f8d4-4e8c-82d5-1a89d3656dd4"
      },
      "source": [
        "In this assignment, you will use a pretrained LLM for question answering on a subset of the Stanford QA Dataset (SQuAD). Here is an example question from SQuAD:\n",
        "\n",
        "> *Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?*\n",
        "\n",
        "Specific domain knowledge to answer questions like this may not be available in the data that the LLM was pre-trained on. As a result, if we simply prompt the the LLM to answer this question, it may tell us that it does not know the answer, or worse, it may hallucinate an incorrect answer. Even if we are lucky and the LLM has have enough information to answer this question from pre-training, but the information may be outdated (the headmaster is likely to change from time to time).\n",
        "\n",
        "Luckily, SQuAD provides a context snippet for each question that may contain the answer, such as\n",
        "\n",
        "> *The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. **The school's headmaster, history professor Juan Pedro Toni**, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.*\n",
        "\n",
        "If we include the context as part of the prompt to the LLM, the model should be able to correctly answer the question (SQuAD contains \"unanswerable questions\", for which the provided context does not provide sufficient information to answer the question -- we will ignore these for the purpose of this assignment).\n",
        "\n",
        "We will consider a scenario in which we don't know which context belongs to which question and we will use **Retrieval Augmented Generation (RAG)** techniques to identify the relevant context from the set of all available contexts.\n",
        "\n",
        "Specifically we will experiment with the following systems:\n",
        "\n",
        "* A baseline \"vanilla QA\" system in which we try to answer the question without any additional context (i.e. using the pre-trained LLM only).\n",
        "* An \"oracle\" system, in which we provide the correct context for each question. This establishes an upper bound for the retrieval approaches.\n",
        "* Two different approaches for retrieving relevant contexts:\n",
        "  * based on token overlap between the question and each context.\n",
        "  * based on cosine similarity between question embeddings and candidate context embeddings (obtained using BERT).\n",
        "    \n",
        "We will evaluate each system using a number of metrics commonly used for QA tasks:\n",
        "* Exact Match (EM), which measures the percentage of predictions that exactly match the ground truth answers.\n",
        "* F1 score, measured on the token overlap between the predicted and ground truth answers.\n",
        "* ROUGE (specifically, ROUGE2)\n",
        "\n",
        "Follow the instructions in this notebook step-by step. Much of the code is provided and just needs to be run, but some sections are marked with todo. Make sure to complete all these sections.\n",
        "\n",
        "\n",
        "Requirements:\n",
        "Access to a GPU is required for this assignment. If you have a recent mac, you can try using mps. Otherwise, I recommend renting a GPU instance through a service like vast.ai or lambdalabs. Google Colab can work in a pinch, but you would have to deal with quotas and it's somewhat easy to lose unsaved work.\n",
        "\n",
        "First, we need to ensure that transformers is installed, as well as the accelerate package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "00a69e38-7395-4361-a3ff-16ed52a89e45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "00a69e38-7395-4361-a3ff-16ed52a89e45",
        "outputId": "bb86ce8d-cf8e-4a86-9b47-1316f76c9441"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ac570bdf-90b7-4dbc-b0df-2377d108f17d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ac570bdf-90b7-4dbc-b0df-2377d108f17d",
        "outputId": "232510f4-42d9-40bd-e985-da85da188f61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b115172-d4cd-448e-a3a2-63d22fe7cbef",
      "metadata": {
        "id": "9b115172-d4cd-448e-a3a2-63d22fe7cbef"
      },
      "source": [
        "Now all the relevant imports should succeed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f8276df6",
      "metadata": {
        "id": "f8276df6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import tqdm\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import re\n",
        "import string\n",
        "import collections\n",
        "\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63708594",
      "metadata": {
        "id": "63708594"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "This section creates the benchmark data we need to evaluate the QA systems. It has already been implemented for you. We recommend that you run it only once, save the benchmark data in a json file and then load it when needed. The following code may not work in Windows. We are providing the pre-generated benchmark data for download as an alternative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "22da799f",
      "metadata": {
        "id": "22da799f"
      },
      "outputs": [],
      "source": [
        "data_dir = \"./squad_data\"\n",
        "if not os.path.exists(data_dir):\n",
        "    os.mkdir(data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "438e31f8-1b85-47bb-9bd3-60dfa76ccea4",
      "metadata": {
        "id": "438e31f8-1b85-47bb-9bd3-60dfa76ccea4"
      },
      "source": [
        "### Downloading the Data and Creating the Benchmark Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "75b74477",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "75b74477",
        "outputId": "b6252f64-1093-4ed3-eae5-441119d8ca09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "training_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\"\n",
        "val_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\"\n",
        "\n",
        "os.system(f\"curl -L {training_url} -o {data_dir}/squad_train.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d634aa0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "d634aa0a",
        "outputId": "946b4169-d01b-4dc0-d949-05b5674944d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of topics: 442\n",
            "==============================\n",
            "For topic \"Beyoncé\"\n",
            "Number of available context paragraphs: 66\n",
            "==============================\n",
            "The first paragraph is:\n",
            "Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
            "==============================\n",
            "The first five question-answer pairs are:\n",
            "Question: When did Beyonce start becoming popular?\n",
            "Answer: in the late 1990s\n",
            "--------------------\n",
            "Question: What areas did Beyonce compete in when she was growing up?\n",
            "Answer: singing and dancing\n",
            "--------------------\n",
            "Question: When did Beyonce leave Destiny's Child and become a solo singer?\n",
            "Answer: 2003\n",
            "--------------------\n",
            "Question: In what city and state did Beyonce  grow up? \n",
            "Answer: Houston, Texas\n",
            "--------------------\n",
            "Question: In which decade did Beyonce become famous?\n",
            "Answer: late 1990s\n",
            "--------------------\n",
            "Question: In what R&B group was she the lead singer?\n",
            "Answer: Destiny's Child\n",
            "--------------------\n",
            "Question: What album made her a worldwide known artist?\n",
            "Answer: Dangerously in Love\n",
            "--------------------\n",
            "Question: Who managed the Destiny's Child group?\n",
            "Answer: Mathew Knowles\n",
            "--------------------\n",
            "Question: When did Beyoncé rise to fame?\n",
            "Answer: late 1990s\n",
            "--------------------\n",
            "Question: What role did Beyoncé have in Destiny's Child?\n",
            "Answer: lead singer\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "# load the raw dataset\n",
        "train_data = json.load(open(f\"{data_dir}/squad_train.json\"))\n",
        "\n",
        "# Some details about the dataset\n",
        "\n",
        "# SQuAD is split up into questions about a number of different topics\n",
        "print(f\"Number of topics: {len(train_data['data'])}\")\n",
        "\n",
        "# Let's explore just one topic. Each topic comes with a number of context paragraphs.\n",
        "print(\"=\"*30)\n",
        "print(f\"For topic \\\"{train_data['data'][0]['title']}\\\"\")\n",
        "print(f\"Number of available context paragraphs: {len(train_data['data'][0]['paragraphs'])}\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "print(\"The first paragraph is:\")\n",
        "print(train_data['data'][0]['paragraphs'][0]['context'])\n",
        "print(\"=\"*30)\n",
        "\n",
        "# Each paragraph comes with a number of question/answer pairs about the text in the paragraph\n",
        "print(\"The first five question-answer pairs are:\")\n",
        "for qa in train_data['data'][0]['paragraphs'][0]['qas'][:10]:\n",
        "    print(f\"Question: {qa['question']}\")\n",
        "    print(f\"Answer: {qa['answers'][0]['text']}\")\n",
        "    print(\"-\"*20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "17faec4a",
      "metadata": {
        "id": "17faec4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "cc337c26-f8e0-461a-e748-3c8e84e7c7a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of paragraphs in the training set: 19035\n",
            "Total number of question-answer pairs in the training set: 130319\n"
          ]
        }
      ],
      "source": [
        "print(\"Total number of paragraphs in the training set:\", sum([len(topic['paragraphs']) for topic in train_data['data']]))\n",
        "print(\"Total number of question-answer pairs in the training set:\", sum([len(paragraph['qas']) for topic in train_data['data'] for paragraph in topic['paragraphs']]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7cf17aff",
      "metadata": {
        "id": "7cf17aff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3a32b9b2-e370-4fd3-8393-97cff69c8671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg number of answers per question: 0.6662190471074824\n",
            "Count of answerable vs unanswerable questions:\n",
            "Answerable questions: 86821 (66.62%)\n",
            "Unanswerable questions: 43498 (33.38%)\n"
          ]
        }
      ],
      "source": [
        "# not all questions are answerable given the information in the paragraph. Part of the original SQuaD 2 task is to identify such\n",
        "# unanswerable questions. We will ignore them for the purpose of this assignment.\n",
        "print(\"Avg number of answers per question:\",\n",
        "      sum([len(qa['answers']) for topic in train_data['data'] for paragraph in topic['paragraphs'] for qa in paragraph['qas']]) /\n",
        "      sum([len(paragraph['qas']) for topic in train_data['data'] for paragraph in topic['paragraphs']]))\n",
        "print(\"Count of answerable vs unanswerable questions:\")\n",
        "answerable_count = 0\n",
        "unanswerable_count = 0\n",
        "for topic in train_data['data']:\n",
        "    for paragraph in topic['paragraphs']:\n",
        "        for qa in paragraph['qas']:\n",
        "            if len(qa['answers']) > 0:\n",
        "                answerable_count += 1\n",
        "            else:\n",
        "                unanswerable_count += 1\n",
        "print(f\"Answerable questions: {answerable_count} ({answerable_count / (answerable_count + unanswerable_count) * 100:.2f}%)\")\n",
        "print(f\"Unanswerable questions: {unanswerable_count} ({unanswerable_count / (answerable_count + unanswerable_count) * 100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0b13dad3",
      "metadata": {
        "id": "0b13dad3"
      },
      "outputs": [],
      "source": [
        "# Finally, create the RAG QA benchmark consisting of 250 answerable questions.\n",
        "\n",
        "# We will use all available context paragraphs for RAG\n",
        "rag_contexts = [paragraph['context'] for topic in train_data['data'] for paragraph in topic['paragraphs']]\n",
        "\n",
        "qa_pairs = []\n",
        "for topic in train_data['data']:\n",
        "    for paragraph in topic['paragraphs']:\n",
        "        for qa in paragraph['qas']:\n",
        "            if len(qa['answers']) > 0:\n",
        "                qa_pairs.append({\n",
        "                    \"question\": qa['question'],\n",
        "                    \"answer\": qa['answers'][0]['text'],\n",
        "                    \"context\": paragraph['context']\n",
        "                })\n",
        "\n",
        "# randomly sample 250 answerable questions for the benchmark\n",
        "import random\n",
        "random.seed(42) # IMPORTANT so everyone is working on the same set of sampled QA pairs\n",
        "sampled_qa_pairs = random.sample(qa_pairs, 250)\n",
        "\n",
        "\n",
        "evaluation_benchmark = {'qas': sampled_qa_pairs,\n",
        "                        'contexts': rag_contexts}\n",
        "random.shuffle(evaluation_benchmark['qas'])\n",
        "random.shuffle(evaluation_benchmark['contexts'])\n",
        "\n",
        "# save the evaluation benchmark to a file\n",
        "json.dump(evaluation_benchmark, open(f\"{data_dir}/rag_qa_benchmark.json\", \"w\"), indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c447a74",
      "metadata": {
        "id": "1c447a74"
      },
      "source": [
        "### Loading the Benchmark Dataset / Understanding the Data Format\n",
        "\n",
        "Use the following code to load the benchmark data from a file. Take a look at the example output to see how the data is structured."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "44d8507e",
      "metadata": {
        "id": "44d8507e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "fd8db11a-aa9d-478f-fefa-d9e7151da097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample RAG contexts:\n",
            "Tajikistan's rivers, such as the Vakhsh and the Panj, have great hydropower potential, and the government has focused on attracting investment for projects for internal use and electricity exports. Tajikistan is home to the Nurek Dam, the highest dam in the world. Lately, Russia's RAO UES energy giant has been working on the Sangtuda-1 hydroelectric power station (670 MW capacity) commenced operations on 18 January 2008. Other projects at the development stage include Sangtuda-2 by Iran, Zerafshan by the Chinese company SinoHydro, and the Rogun power plant that, at a projected height of 335 metres (1,099 ft), would supersede the Nurek Dam as highest in the world if it is brought to completion. A planned project, CASA 1000, will transmit 1000 MW of surplus electricity from Tajikistan to Pakistan with power transit through Afghanistan. The total length of transmission line is 750 km while the project is planned to be on Public-Private Partnership basis with the support of WB, IFC, ADB and IDB. The project cost is estimated to be around US$865 million. Other energy resources include sizable coal deposits and smaller reserves of natural gas and petroleum.\n",
            "--------------------\n",
            "Two years later, the Emperor Valens, who favored the Arian position, in his turn exiled Athanasius. This time however, Athanasius simply left for the outskirts of Alexandria, where he stayed for only a few months before the local authorities convinced Valens to retract his order of exile. Some early reports state that Athanasius spent this period of exile at his family's ancestral tomb in a Christian cemetery. It was during this period, the final exile, that he is said to have spent four months in hiding in his father's tomb. (Soz., \"Hist. Eccl.\", VI, xii; Soc., \"Hist. Eccl.\", IV, xii).\n",
            "--------------------\n",
            "==============================\n",
            "Sample RAG QA pairs:\n",
            "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
            "Answer: professor Juan Pedro Toni\n",
            "--------------------\n",
            "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
            "Answer: about six to four\n",
            "--------------------\n",
            "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
            "Answer: 1890s\n",
            "--------------------\n",
            "Question: When did devolution in the UK begin?\n",
            "Answer: 1914\n",
            "--------------------\n",
            "Question: Treating the mitrailleuse like what rendered it far less effective\n",
            "Answer: artillery\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "# load the benchmark and display some samples\n",
        "evaluation_benchmark = json.load(open(f\"{data_dir}/rag_qa_benchmark.json\"))\n",
        "\n",
        "print(\"Sample RAG contexts:\")\n",
        "for context in evaluation_benchmark['contexts'][:2]:\n",
        "    print(context)\n",
        "    print(\"-\"*20)\n",
        "print(\"=\"*30)\n",
        "print(\"Sample RAG QA pairs:\")\n",
        "for qa in evaluation_benchmark['qas'][:5]:\n",
        "    print(f\"Question: {qa['question']}\")\n",
        "    print(f\"Answer: {qa['answer']}\")\n",
        "    print(\"-\"*20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbfa58b5-2b8a-41c7-b2f5-efaeb6d2542e",
      "metadata": {
        "id": "fbfa58b5-2b8a-41c7-b2f5-efaeb6d2542e"
      },
      "source": [
        "The `evaluation_benchmark` is a dictionary with two keys:\n",
        "* `evaluation_benchmark['qas']`  provides a list of *qa_items* (see below).\n",
        "* `evaluation_benchmark['contexts']` provides a list of available candidate contexts. Note that this includes all contexts from SQuAD, not just the ones for the 250 questions we sampled for the benchmark.\n",
        "\n",
        "Each *qa_item* is a dictionary with the following keys:\n",
        "* `qa_item['question']` is the question string\n",
        "* `qa_item['answer']` is the target answer string\n",
        "* `qa_item['context']` is the gold context for this question\n",
        "\n",
        "For example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "32bce248-2f3f-47a9-8940-aecec5f9f6cb",
      "metadata": {
        "id": "32bce248-2f3f-47a9-8940-aecec5f9f6cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0a35ad74-64f9-432a-e085-902e2d2c9ce6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "250"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "qa_items = evaluation_benchmark['qas']\n",
        "len(qa_items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d428ec2b-98d7-4760-bad7-9e7ef3d01d52",
      "metadata": {
        "id": "d428ec2b-98d7-4760-bad7-9e7ef3d01d52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "98168e14-89a9-4be7-d661-6ff21cb0f958"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "qa_item = qa_items[0]\n",
        "qa_item['question']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2ede20e1-82be-46f4-9fc6-7da84835f6af",
      "metadata": {
        "id": "2ede20e1-82be-46f4-9fc6-7da84835f6af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "56472813-7c1a-420d-9573-991c31a88439"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'professor Juan Pedro Toni'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "qa_item['answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "94856286-ac1f-4256-a279-b013202e84c7",
      "metadata": {
        "id": "94856286-ac1f-4256-a279-b013202e84c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "7cd7a226-809c-4be8-c6fb-f042dfc75c8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. The school's headmaster, history professor Juan Pedro Toni, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "qa_item['context']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21c6cd6b",
      "metadata": {
        "id": "21c6cd6b"
      },
      "source": [
        "## Part 1 - Question Answering Evaluation Functions\n",
        "\n",
        "In this section. we will define a number of evaluation functions that measure the quality of the QA output, compared to a single target answer for each question.\n",
        "\n",
        "Because the evaluation will happen at a token leve, we will perform some simple pre-processing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1fc9fc26",
      "metadata": {
        "id": "1fc9fc26"
      },
      "outputs": [],
      "source": [
        "def normalize_answer(s):\n",
        "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "  def remove_articles(text):\n",
        "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "    return re.sub(regex, ' ', text)\n",
        "  def white_space_fix(text):\n",
        "    return ' '.join(text.split())\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def get_tokens(s):\n",
        "  if not s: return []\n",
        "  return normalize_answer(s).split()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d64fe47f-bbd6-4f48-b641-56d279c05ab8",
      "metadata": {
        "id": "d64fe47f-bbd6-4f48-b641-56d279c05ab8"
      },
      "source": [
        "First, Exact Match (EM) measures the percentage of predictions that match any one of the ground truth answers exactly after normalization.\n",
        "The following function returns 1 if the predicted answer is correct and 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "37139082-6480-47b1-b712-b2c228f08c78",
      "metadata": {
        "id": "37139082-6480-47b1-b712-b2c228f08c78"
      },
      "outputs": [],
      "source": [
        "def compute_exact(a_gold, a_pred):\n",
        "  return int(normalize_answer(a_gold) == normalize_answer(a_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d41af10d-f971-46ff-b917-709644d22f8a",
      "metadata": {
        "id": "d41af10d-f971-46ff-b917-709644d22f8a"
      },
      "source": [
        "The next function calculates the $F_1$ score of the set of predicted tokens against the set of target tokens.\n",
        "$F_1$ is the harmonic mean of precision and recall, providing a balance between the two. Specifically\n",
        "\n",
        "$F_1 = \\frac{2 \\times \\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}$\n",
        "\n",
        "where $\\text{precision}$ is the fraction of predicted tokens that also appear in the target and $\\text{recall}$ is the fraction of target tokens that also appear in the prediction.\n",
        "\n",
        "**TODO**: Write the function compute_f1(a_gold, a_pred) that returns the F1 score as defined above. It should work similar to the compute_exact method above. Test your function on a sample answer and prediction to verify that it works correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "f5c58004-6eb5-434c-9436-ae9adaf9799f",
      "metadata": {
        "id": "f5c58004-6eb5-434c-9436-ae9adaf9799f"
      },
      "outputs": [],
      "source": [
        "def compute_f1(a_gold, a_pred): # Complete the function\n",
        "  gold_tokens = get_tokens(a_gold)\n",
        "  pred_tokens = get_tokens(a_pred)\n",
        "\n",
        "  if len(gold_tokens) == 0 or len(pred_tokens) == 0:\n",
        "    return 0.0\n",
        "\n",
        "  common = set(gold_tokens) & set(pred_tokens)\n",
        "  if len(common) == 0:\n",
        "    return 0.0\n",
        "\n",
        "  precision = len(common) / len(pred_tokens)\n",
        "  recall = len(common) / len(gold_tokens)\n",
        "\n",
        "  f1 = 2 * precision * recall / (precision + recall)\n",
        "  return f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "97bea5d9-6e0e-4c49-a277-c1f9259078a2",
      "metadata": {
        "id": "97bea5d9-6e0e-4c49-a277-c1f9259078a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "82355ee1-6fa1-412e-9478-f5ddc4089a2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:\n",
            "Reference: London | Predicted: London, capital of England\n",
            "Normalized:\n",
            "Reference: london | Predicted: london capital of england\n",
            "Exact Match: 0\n",
            "F1 Score: 0.4\n",
            "----------------------------------------\n",
            "Original:\n",
            "Reference: The capital of England is London. | Predicted: London, capital of England\n",
            "Normalized:\n",
            "Reference: capital of england is london | Predicted: london capital of england\n",
            "Exact Match: 0\n",
            "F1 Score: 0.888888888888889\n",
            "----------------------------------------\n",
            "Original:\n",
            "Reference: London is the capital city of England. | Predicted: London, capital of England\n",
            "Normalized:\n",
            "Reference: london is capital city of england | Predicted: london capital of england\n",
            "Exact Match: 0\n",
            "F1 Score: 0.8\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Test your function\n",
        "reference_answers = [\"London\", \"The capital of England is London.\", \"London is the capital city of England.\"]\n",
        "predicted_answers = [\"London, capital of England\"] * len(reference_answers)\n",
        "\n",
        "for ref, pred in zip(reference_answers, predicted_answers):\n",
        "    print(f\"Original:\")\n",
        "    print(f\"Reference: {ref} | Predicted: {pred}\")\n",
        "    print(f\"Normalized:\")\n",
        "    print(f\"Reference: {normalize_answer(ref)} | Predicted: {normalize_answer(pred)}\")\n",
        "    print(\"Exact Match:\", compute_exact(normalize_answer(ref), normalize_answer(pred)))\n",
        "    print(\"F1 Score:\", compute_f1(normalize_answer(ref), normalize_answer(pred)))\n",
        "    print(\"-\"*40)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0749c7ec-0bce-420b-a614-9374ebbb0ef8",
      "metadata": {
        "id": "0749c7ec-0bce-420b-a614-9374ebbb0ef8"
      },
      "source": [
        "Finally, we are also want to compute ROUGE-2 scores (which extends the F1 score above to 2-grams). We can use the `rouge_score` package to do this for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "d5c82b5e-f256-4bdf-a4b5-9eac65f7e87d",
      "metadata": {
        "id": "d5c82b5e-f256-4bdf-a4b5-9eac65f7e87d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1ea5019d-425e-48c7-d9c4-94692b25e54f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=2f0c5d973b1b4f54088344c54de15a7c266cc9a41c1eb0f84827aace661d1160\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a5ef6e0f",
      "metadata": {
        "id": "a5ef6e0f"
      },
      "outputs": [],
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "rouge_scorer = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=False)\n",
        "\n",
        "def compute_rouge2(a_gold, a_pred):\n",
        "    if not a_gold or not a_pred:\n",
        "        return 0.0\n",
        "    scores = rouge_scorer.score(a_gold.lower(), a_pred.lower())\n",
        "    return scores['rouge2'].fmeasure"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d7e68d8-9760-486b-a07b-78c24efd636f",
      "metadata": {
        "id": "9d7e68d8-9760-486b-a07b-78c24efd636f"
      },
      "source": [
        "Let's test the metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7f2cd8a9",
      "metadata": {
        "id": "7f2cd8a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5fce59c3-687e-4b87-b412-af000d755181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Answers:\n",
            "Original:\n",
            "Reference: London | Predicted: London, capital of England\n",
            "Normalized:\n",
            "Reference: london | Predicted: london capital of england\n",
            "Exact Match: 0\n",
            "F1 Score: 0.4\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Original:\n",
            "Reference: The capital of England is London. | Predicted: London, capital of England\n",
            "Normalized:\n",
            "Reference: capital of england is london | Predicted: london capital of england\n",
            "Exact Match: 0\n",
            "F1 Score: 0.888888888888889\n",
            "ROUGE-2 F1-score: 0.5714285714285715\n",
            "----------------------------------------\n",
            "Original:\n",
            "Reference: London is the capital city of England. | Predicted: London, capital of England\n",
            "Normalized:\n",
            "Reference: london is capital city of england | Predicted: london capital of england\n",
            "Exact Match: 0\n",
            "F1 Score: 0.8\n",
            "ROUGE-2 F1-score: 0.25\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "reference_answers = [\"London\", \"The capital of England is London.\", \"London is the capital city of England.\"]\n",
        "predicted_answers = [\"London, capital of England\"] * len(reference_answers)\n",
        "\n",
        "print(\"Normalized Answers:\")\n",
        "for ref, pred in zip(reference_answers, predicted_answers):\n",
        "    print(f\"Original:\")\n",
        "    print(f\"Reference: {ref} | Predicted: {pred}\")\n",
        "    print(f\"Normalized:\")\n",
        "    print(f\"Reference: {normalize_answer(ref)} | Predicted: {normalize_answer(pred)}\")\n",
        "    print(\"Exact Match:\", compute_exact(normalize_answer(ref), normalize_answer(pred)))\n",
        "    print(\"F1 Score:\", compute_f1(normalize_answer(ref), normalize_answer(pred)))\n",
        "    print(\"ROUGE-2 F1-score:\", compute_rouge2(normalize_answer(ref), normalize_answer(pred)))\n",
        "    print(\"-\"*40)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87b0f4bc",
      "metadata": {
        "id": "87b0f4bc"
      },
      "source": [
        "## Part 2 - Vanilla Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f53f620b-6326-498e-80cf-c4886c1b3edd",
      "metadata": {
        "id": "f53f620b-6326-498e-80cf-c4886c1b3edd"
      },
      "source": [
        "In this part, we will use an off-the-shelf pretrained LLM and attempt to answer the questions from its pretraining knowledge only.\n",
        "To make things simple, we will use the huggingface transformer pipeline abstraction. The pipeline will download the model and parameters for us on creation. When we pass an input prompt to the pipeline, it will automatically perform preprocessing (tokenization), inference, and postprocessing (removing EOS markers and padding).\n",
        "\n",
        "### Loading the LLM\n",
        "The LLM we will use is the 1B version of the instruction tuned OLMo2 model. OLMo is an open source language model created by Allen AI and the University of Washington. Unlike other open source models, OLMo is also open data. You can read more about it here: https://huggingface.co/allenai/OLMo-2-0425-1B-Instruct and here https://allenai.org/olmo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "bf584e78",
      "metadata": {
        "id": "bf584e78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418,
          "referenced_widgets": [
            "bacbf0d5aa41468898c728e8cbfaa0bd",
            "66b7b83e58894830a3bdc8ac77c54413",
            "1aec602be1e142c9a3e408aad2618554",
            "5c9ec76ff16a458bb9ead5c0ce6c3482",
            "385bdfb4a228467288b5a53f03956642",
            "9e015f285f834156b553734d34c0e4d5",
            "a346728ded554d5cba9c4e0c803d1b59",
            "e8eb55be214b4373a8b9687852baa5cf",
            "b3872cd3aa4b404c87225184d9f5d37f",
            "0e0eff1dbe3748a392c001e5261c856b",
            "9abd56f1ec654e2cb206dc7c9fcbdaae",
            "adbdad65ba814a7f91c1fd6a47158f58",
            "bed6e171f40140a7bb23546e10faaa57",
            "f24f128689234e87bd6bee26cd3df6cf",
            "f16b9bfa200342a3ac7bd370a59bbaf2",
            "f26778c56f304da09d83cfa68c14471e",
            "49f8af4cb1b14c939f89ee243c99bb10",
            "f81dbda99bc142faae3cd32e7078cf7b",
            "74908ad895234c048a017fef401ad992",
            "009a8b6439fc446da361b01de533587b",
            "33a869015a45439fa5a71265e6b7ee6b",
            "bb7d6198a1e54de8a8f10fb48e248c89",
            "cec7a23950634a10ae2770a83a22d4a4",
            "39627779680e45639f63b81295a52434",
            "4bd9a6052bfa413183c8e155b8d62950",
            "473710553d9442edaedf74279b34677e",
            "8e9cf6a306a447cc921b1c354e0eb48b",
            "a9f4caf6542640789a282cfcbe553a85",
            "2fe5f5a752794780b5f3962be13937c3",
            "c9fa367e15f44ff9957f5541238a282e",
            "8613b9cf566c4b2e95730a19c19adbf2",
            "41ff57fd27e54858b605384861111c7d",
            "b14b6e2b9e7d4ef5a635c4bdf7373c9a",
            "8993eb9a4578435d8324be0905ec3815",
            "52fd0527d4624155aa60ae36ea969973",
            "40ef0dad267c4adcbc90134c10493dfb",
            "5f08a54f832a41a0942a126d9c6a94c1",
            "9645fae93e834d9bb3d557cd6419e613",
            "ed5e33ee7a9542eab3e4e5a8496fbc2b",
            "ecd9371c0d7d45cea1bcc63e10649564",
            "380c5a9c507746da883de66a8a83227a",
            "c9e1d00f5fdf4fa6a86c2ab24a984851",
            "a2f60010bd334ed7956069230e967fc1",
            "4bdfa4a10f7c4db5b1abf1463d3da65f",
            "a0c8b902dcc3493d8ff32fa96cc1a248",
            "d5fe363e1c854f3ca9163e8e2a095ccc",
            "9fc169ffdd9f4e4c903dbba08f70a724",
            "99500cb2196a4c1dba1cb2b5f3050880",
            "acbe2492798845b085f3a1d5599d6759",
            "f1992dfd99c64d4dba9dfc5a60f4b8d3",
            "b2fb4ff5e2da44efad9c57f7a7e4d376",
            "1b5fa2a6452c44a696773171249aaab4",
            "8e0038054b81407b81498d50c4c7b60a",
            "037746ed4b0448dda0bb7d0f830bb834",
            "492f6067409e4cd3ab250a1bc8316d30",
            "eaad9a77e5a7406694b03d4b7c987263",
            "7a1710b475294e2590438ce7c47e57d1",
            "9d68823b026d4f1280981b21160d3996",
            "39e3299e26c144c1abdc4ee10017a1b0",
            "83afe6ab85854dfb992651044a1049ae",
            "583a2c447a184a7ead01b5e0d6f421ad",
            "704fa2ed86774180b02ab9eab2d85f47",
            "4be93ba8b7874aabbc517037ed838d28",
            "bf0d772dd1f6415cbca20c08fb87e70a",
            "5c84e47bcb3347e98350d6be60e0dc69",
            "3013ae45bab34f45b453c17e3efaeea5",
            "4dc29f35310447efb710946feaa5aca5",
            "ed9036e0fc554e19bfa1f00e2149d71c",
            "1885b4f7cf764685a00945b0a29c62c0",
            "cc3cdd5cd14b4fa99c2651d4573351d4",
            "8e54992a77fd483eb387a587e5318cbe",
            "49e0eecead9846f79369c527ca7e0938",
            "a813b8a372d34d3a84c954ee5bc6dfef",
            "25c9e65639d145f382a77823a706dee2",
            "e03e761b6f994fc6be3d0bb7ee31234d",
            "9c5ecf66f1214125b6b87584e48878fb",
            "784237dbc0f44096a17665780f3affd4",
            "5f5c238b31a544abbbc66e7db43a7996",
            "64ec545d84924e6aa67bcb460d9084f1",
            "eafeb3762b8e41a5bd958f1ff25c8090",
            "5b8f39543cea430e8b2135d1a061616b",
            "22c30e860b034497890b31e97077f550",
            "801fe698b3244e0987da734b60a8038a",
            "d69ec1575da441899a58e917b5c55d9c",
            "abc520958046430a8c439fe93f674fa9",
            "2faf73e4aeb74568bdad860d6d1824e0",
            "3289ed4a189248e292d39371b91cbfb4",
            "6289b4af693c42d7980cced4a770ffda"
          ]
        },
        "outputId": "45948b19-be13-4d24-e551-d11862955016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bacbf0d5aa41468898c728e8cbfaa0bd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.97G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "adbdad65ba814a7f91c1fd6a47158f58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/121 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cec7a23950634a10ae2770a83a22d4a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8993eb9a4578435d8324be0905ec3815"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0c8b902dcc3493d8ff32fa96cc1a248"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eaad9a77e5a7406694b03d4b7c987263"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4dc29f35310447efb710946feaa5aca5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/581 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f5c238b31a544abbbc66e7db43a7996"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        }
      ],
      "source": [
        "qa_model = \"allenai/OLMo-2-0425-1B-Instruct\"\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Check which GPU device to use. Note, this will likely NOT work on a CPU.\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=qa_model,\n",
        "    dtype=torch.bfloat16,\n",
        "    device_map=device,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "948b1078-2f98-4ca3-b070-f29854ecfd23",
      "metadata": {
        "id": "948b1078-2f98-4ca3-b070-f29854ecfd23"
      },
      "source": [
        "We can now pass a prompt to the model and retreive the completed answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "75e256c3-e801-43fc-af45-ead3d31da487",
      "metadata": {
        "id": "75e256c3-e801-43fc-af45-ead3d31da487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d6110e29-200e-45a5-a8ff-37d84a8efe35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \"My favorite thing to do in fall is bake homemade bread. Would you like to come over for dinner tonight?\\n\\nWould you like to know what we're having? I've been experimenting with some new recipes and I'd be thrilled to show you some of my favorites.\\n\\nPlease tell me about your favorite fall dishes.\\n\\nWould you like to try some of my homemade cranberry sauce with the roasted turkey? I've been making it every year and it's always a hit.\\n\\nFeel free to ask me anything else about fall baking or cooking. It's a wonderful season!\"}]\n"
          ]
        }
      ],
      "source": [
        "prompt = \"My favorite thing to do in fall is\"\n",
        "output = pipe(prompt,\n",
        "              max_new_tokens=128,\n",
        "              do_sample=True, # set to False for greedy decoding below\n",
        "              pad_token_id=pipe.tokenizer.eos_token_id)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e790a89-bdad-4bd7-890d-79cc564c7f76",
      "metadata": {
        "id": "5e790a89-bdad-4bd7-890d-79cc564c7f76"
      },
      "source": [
        "We can skip the prompt that is repeated in the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "73e6c0c3-d45a-4cbd-977d-269274c160ab",
      "metadata": {
        "id": "73e6c0c3-d45a-4cbd-977d-269274c160ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "e1b14bcf-8c33-4a8e-8f08-930991da2b77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"bake homemade bread. Would you like to come over for dinner tonight?\\n\\nWould you like to know what we're having? I've been experimenting with some new recipes and I'd be thrilled to show you some of my favorites.\\n\\nPlease tell me about your favorite fall dishes.\\n\\nWould you like to try some of my homemade cranberry sauce with the roasted turkey? I've been making it every year and it's always a hit.\\n\\nFeel free to ask me anything else about fall baking or cooking. It's a wonderful season!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "output[0]['generated_text'][len(prompt):].strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a9c1915-5ee3-4a78-ad82-557154486eae",
      "metadata": {
        "id": "7a9c1915-5ee3-4a78-ad82-557154486eae"
      },
      "source": [
        "### Using the LLM for Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d61c365-c1ef-4bec-a991-e388e375e61b",
      "metadata": {
        "id": "3d61c365-c1ef-4bec-a991-e388e375e61b"
      },
      "source": [
        "**TODO:** Write a function `vanilla_qa(qa_item)` that take a qa_item in the format described above, inserts the question (and only the question!) into a suitable prompt, passes the prompt to the LLM and then returns the answer as a string.\n",
        "\n",
        "A prompt might look like this, but will need a bit of prompt engineering to make it work well.\n",
        "\n",
        "> *Answer the following question concisely.*\n",
        ">\n",
        "> *Question: Who played he lead role in Alien?*\n",
        ">\n",
        "> *Answer:*\n",
        "\n",
        "Once you have a basic version of the vanilla QA system you can tune the prompt (see below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "26b94d1b-a91a-495e-9589-f241505c429d",
      "metadata": {
        "id": "26b94d1b-a91a-495e-9589-f241505c429d"
      },
      "outputs": [],
      "source": [
        "def vanilla_qa(qa_item): # Complete this function\n",
        "    prompt = (\n",
        "        \"As a factual question answering system, you are going to answer a question.\\n\"\n",
        "        \"Answer the question using as few words as possible.\\n\"\n",
        "        \"Do not include articles (a, an, the), punctuation, or explanations.\\n\\n\"\n",
        "        \"Example:\\n\"\n",
        "        \"Question: What year is this year?\\n\"\n",
        "        \"Answer: 2025\\n\\n\"\n",
        "        f\"Question: {qa_item['question']}\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n",
        "    output = pipe(prompt,\n",
        "              max_new_tokens=128,\n",
        "              do_sample=True, # set to False for greedy decoding below\n",
        "              pad_token_id=pipe.tokenizer.eos_token_id)\n",
        "    answer = output[0]['generated_text'][len(prompt):].strip()\n",
        "    return answer\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4a406ff-4a0f-4641-a34c-eb5559779dd4",
      "metadata": {
        "id": "a4a406ff-4a0f-4641-a34c-eb5559779dd4"
      },
      "source": [
        "The following code should return an answer (but possibly not the right one) to the first question in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "2e4df6c1-8533-4744-9f52-2aa48c088eb0",
      "metadata": {
        "id": "2e4df6c1-8533-4744-9f52-2aa48c088eb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e1cff0f6-263c-4ec6-d97b-d0c5ee7e4332"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'professor Juan Pedro Toni'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "qa_item = evaluation_benchmark['qas'][0]\n",
        "qa_item['question']\n",
        "qa_item['answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "d9e4cfb5-4eb6-41fb-9aef-aac01451e0f3",
      "metadata": {
        "id": "d9e4cfb5-4eb6-41fb-9aef-aac01451e0f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "647e76ae-2cf5-47c9-c14b-9332d1a7742f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ian K Folly'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 101
        }
      ],
      "source": [
        "vanilla_qa(qa_item) # inspect the item"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "823453c2-b9ef-490f-ace8-e411348da498",
      "metadata": {
        "id": "823453c2-b9ef-490f-ace8-e411348da498"
      },
      "source": [
        "And the following function evaluates the performance of your `vanilla_qa` function on a list of qa_items."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "f4a6c234",
      "metadata": {
        "id": "f4a6c234"
      },
      "outputs": [],
      "source": [
        "def evaluate_qa(qa_function, qa_items, verbose=False):\n",
        "    results = []\n",
        "\n",
        "\n",
        "    for i, qa_item in tqdm.tqdm(enumerate(qa_items), desc=\"Evaluating QA instances\", total=len(qa_items)):\n",
        "\n",
        "        question = qa_item['question']\n",
        "        answer = qa_item['answer']\n",
        "        context = qa_item['context']\n",
        "\n",
        "        predicted_answer = qa_function(qa_item)\n",
        "\n",
        "        exact_match = compute_exact(answer, predicted_answer)\n",
        "        f1_score = compute_f1(answer, predicted_answer)\n",
        "        rouge2_f1 = compute_rouge2(answer, predicted_answer)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Q: {question}\")\n",
        "            print(f\"Gold Answer: {answer}\")\n",
        "            print(f\"Predicted Answer: {answer}\")\n",
        "            print(f\"Exact Match: {exact_match}, F1 Score: {f1_score}\")\n",
        "            print(f\"ROUGE-2 F1 Score: {rouge2_f1}\")\n",
        "            print(\"-\"*40)\n",
        "\n",
        "        results.append({\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"predicted_answer\": predicted_answer,\n",
        "            \"context\": context if context else None,\n",
        "            \"exact_match\": exact_match,\n",
        "            \"f1_score\": f1_score,\n",
        "            \"rouge2_f1\": rouge2_f1\n",
        "        })\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "dc60afcc",
      "metadata": {
        "id": "dc60afcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "85f16adb-8c08-4fda-cf25-cd80afa92066"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating QA instances: 100%|██████████| 250/250 [00:39<00:00,  6.30it/s]\n"
          ]
        }
      ],
      "source": [
        "vanilla_evaluation_results = evaluate_qa(vanilla_qa, evaluation_benchmark['qas'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "a9b4c4fe-51b2-40bb-b2c0-91dd572fbc12",
      "metadata": {
        "id": "a9b4c4fe-51b2-40bb-b2c0-91dd572fbc12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "945fe1e7-2a26-4263-a33b-0dd5cefe4697"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?',\n",
              " 'answer': 'professor Juan Pedro Toni',\n",
              " 'predicted_answer': 'Brendan Comer',\n",
              " 'context': \"The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. The school's headmaster, history professor Juan Pedro Toni, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.\",\n",
              " 'exact_match': 0,\n",
              " 'f1_score': 0.0,\n",
              " 'rouge2_f1': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ],
      "source": [
        "vanilla_evaluation_results[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f869c4dc-5c55-4003-8bed-c68966fdbc98",
      "metadata": {
        "id": "f869c4dc-5c55-4003-8bed-c68966fdbc98"
      },
      "source": [
        "The function returns a list of evaluation results, one dictionary for each qa item."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a658f34e-1b56-4c71-b4bb-54e88b6df3b6",
      "metadata": {
        "id": "a658f34e-1b56-4c71-b4bb-54e88b6df3b6"
      },
      "source": [
        "Finally, the `present_results` function aggregates the results for the various qa items and prints the overall result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "id": "d72c6f4c",
      "metadata": {
        "id": "d72c6f4c"
      },
      "outputs": [],
      "source": [
        "def present_results(eval_results, exp_name=\"\"):\n",
        "    print(f\"{exp_name} Evaluation Results:\")\n",
        "    exact_matches = [res['exact_match'] for res in eval_results]\n",
        "    f1_scores = [res['f1_score'] for res in eval_results]\n",
        "    rouge2_f1 = [res['rouge2_f1'] for res in eval_results]\n",
        "    print(f\"Exact Match: {sum(exact_matches) / len(exact_matches) * 100:.2f}%\")\n",
        "    print(f\"F1 Score: {sum(f1_scores) / len(f1_scores) * 100:.2f}%\")\n",
        "    print(f\"ROUGE2 F1: {sum(rouge2_f1) / len(rouge2_f1) * 100:.2f}%\")\n",
        "\n",
        "    # print out some evaluation results\n",
        "    for res in eval_results[:5]:\n",
        "        print(f\"Question: {res['question']}\")\n",
        "        print(f\"Gold Answer: {res['answer']}\")\n",
        "        print(f\"Predicted Answer: {res['predicted_answer']}\")\n",
        "        print(f\"Exact Match: {res['exact_match']}, F1 Score: {res['f1_score']}\")\n",
        "        print(\"ROUGE-2 F1-score:\", res['rouge2_f1'])\n",
        "        print(\"-\"*40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "cb2bf077-4292-4736-bf04-146f846daa7a",
      "metadata": {
        "id": "cb2bf077-4292-4736-bf04-146f846daa7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b5a5061c-9e3b-443a-abc9-139c713d17a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vanilla QA Evaluation Results:\n",
            "Exact Match: 6.40%\n",
            "F1 Score: 12.12%\n",
            "ROUGE2 F1: 2.26%\n",
            "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
            "Gold Answer: professor Juan Pedro Toni\n",
            "Predicted Answer: Brendan Comer\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
            "Gold Answer: about six to four\n",
            "Predicted Answer: 3:2\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
            "Gold Answer: 1890s\n",
            "Predicted Answer: 1895\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did devolution in the UK begin?\n",
            "Gold Answer: 1914\n",
            "Predicted Answer: Scotland\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: Treating the mitrailleuse like what rendered it far less effective\n",
            "Gold Answer: artillery\n",
            "Predicted Answer: French\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "present_results(vanilla_evaluation_results, \"Vanilla QA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e785b7b5-bb3b-49da-9699-e25c18a014df",
      "metadata": {
        "id": "e785b7b5-bb3b-49da-9699-e25c18a014df"
      },
      "source": [
        "**TODO:** Experiment with the prompt template and try to achieve an Exact Match score of at least 5%. You may want to try including an example in the prompt (single-shot prompting)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bab2e479",
      "metadata": {
        "id": "bab2e479"
      },
      "source": [
        "## Part 3 - Oracle Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c011bec1-e942-46d9-a193-4c8944e8b3f9",
      "metadata": {
        "id": "c011bec1-e942-46d9-a193-4c8944e8b3f9"
      },
      "source": [
        "We will now establish an upper bound for a retrieval augmented QA system by providing the correct (\"gold\") context for each question as part of the prompt. These contexts are available as part of each qa_item in the evaluation_benchmark['qas'] dictionary.\n",
        "\n",
        "**TODO**: Write a function `oracle_qa(qa_item)` that takes in a qa_item, inserts both the question **and** the gold context into a prompt template, then passes the prompt to the LLM and returns the answer. The function should behave like the `vanilla_qa` function above, so that we can evaluate it using the same evaluation steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "c69a364b-2cbf-4206-8bac-d806d7688a9c",
      "metadata": {
        "id": "c69a364b-2cbf-4206-8bac-d806d7688a9c"
      },
      "outputs": [],
      "source": [
        "def oracle_qa(qa_item): # Write this function\n",
        "  prompt = (\n",
        "      \"As a factual question answering system, you are going to answer question.\\n\"\n",
        "      \"Answer the question using as few words as possible.\\n\"\n",
        "      \"Do not include articles (a, an, the), punctuation, or explanations.\\n\"\n",
        "      f\"Context: {qa_item['context']}\"\n",
        "      f\"Question: {qa_item['question']}\\n\"\n",
        "      \"Answer:\"\n",
        "  )\n",
        "\n",
        "  output = pipe(prompt,\n",
        "            max_new_tokens=128,\n",
        "            do_sample=True, # set to False for greedy decoding below\n",
        "            pad_token_id=pipe.tokenizer.eos_token_id)\n",
        "\n",
        "  answer = output[0]['generated_text'][len(prompt):].strip()\n",
        "  return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db7aeb37-dc13-4e30-be8d-eca28c6376fa",
      "metadata": {
        "id": "db7aeb37-dc13-4e30-be8d-eca28c6376fa"
      },
      "source": [
        "**TODO**: run the `evaluate_qa` function on your `oracle_qa` function and display the results. You should see Exact Match scores above 50% (if not, tinker with the prompt template)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "6cf95161-ebfc-4b99-a263-5ad8f230f298",
      "metadata": {
        "id": "6cf95161-ebfc-4b99-a263-5ad8f230f298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "24c199c1-3516-46ad-febd-e66af24d98f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating QA instances: 100%|██████████| 250/250 [00:32<00:00,  7.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Evaluation Results:\n",
            "Exact Match: 56.80%\n",
            "F1 Score: 68.60%\n",
            "ROUGE2 F1: 32.01%\n",
            "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
            "Gold Answer: professor Juan Pedro Toni\n",
            "Predicted Answer: Juan Pedro Toni\n",
            "Exact Match: 0, F1 Score: 0.8571428571428571\n",
            "ROUGE-2 F1-score: 0.8\n",
            "----------------------------------------\n",
            "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
            "Gold Answer: about six to four\n",
            "Predicted Answer: 6 4\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
            "Gold Answer: 1890s\n",
            "Predicted Answer: 1890s\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did devolution in the UK begin?\n",
            "Gold Answer: 1914\n",
            "Predicted Answer: 1914\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: Treating the mitrailleuse like what rendered it far less effective\n",
            "Gold Answer: artillery\n",
            "Predicted Answer: artillery\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "oracle_evaluation_results = evaluate_qa(oracle_qa, evaluation_benchmark['qas'])\n",
        "present_results(oracle_evaluation_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bee3d53",
      "metadata": {
        "id": "6bee3d53"
      },
      "source": [
        "## Part 4 - Retrieval-Augmented Question Answering - Word Overlap"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1ae47c1-230c-4d39-9670-c82f18ac3f4c",
      "metadata": {
        "id": "d1ae47c1-230c-4d39-9670-c82f18ac3f4c"
      },
      "source": [
        "Next, we will experiment with various approaches for retrieving relevant contexts from the set of available contexts. We first get the list of all 19035 available candidate contexts from the evaluation_benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "26c0c460-2b20-4ea4-855c-aa8840893be6",
      "metadata": {
        "id": "26c0c460-2b20-4ea4-855c-aa8840893be6"
      },
      "outputs": [],
      "source": [
        "candidate_contexts = evaluation_benchmark[\"contexts\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "804e3f34-de25-47f0-a33f-3234739cacf4",
      "metadata": {
        "id": "804e3f34-de25-47f0-a33f-3234739cacf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "fd1656f7-8982-41f4-aad5-e5a032e74c43"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19035"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ],
      "source": [
        "len(candidate_contexts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "cfb4e746-6909-45ee-b6a6-4c3bc31949c5",
      "metadata": {
        "id": "cfb4e746-6909-45ee-b6a6-4c3bc31949c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "f201c30f-202c-4316-c555-fab56d8ff21e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Tajikistan's rivers, such as the Vakhsh and the Panj, have great hydropower potential, and the government has focused on attracting investment for projects for internal use and electricity exports. Tajikistan is home to the Nurek Dam, the highest dam in the world. Lately, Russia's RAO UES energy giant has been working on the Sangtuda-1 hydroelectric power station (670 MW capacity) commenced operations on 18 January 2008. Other projects at the development stage include Sangtuda-2 by Iran, Zerafshan by the Chinese company SinoHydro, and the Rogun power plant that, at a projected height of 335 metres (1,099 ft), would supersede the Nurek Dam as highest in the world if it is brought to completion. A planned project, CASA 1000, will transmit 1000 MW of surplus electricity from Tajikistan to Pakistan with power transit through Afghanistan. The total length of transmission line is 750 km while the project is planned to be on Public-Private Partnership basis with the support of WB, IFC, ADB and IDB. The project cost is estimated to be around US$865 million. Other energy resources include sizable coal deposits and smaller reserves of natural gas and petroleum.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 115
        }
      ],
      "source": [
        "candidate_contexts[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f9bf018-6240-436f-b3e5-eef3f0be16d9",
      "metadata": {
        "id": "3f9bf018-6240-436f-b3e5-eef3f0be16d9"
      },
      "source": [
        "### Token Overlap Retriever\n",
        "Let's first experiment with a simple retriever based on word overlap. Given a question, we measure how many of its tokens appear in each of the candidate contexts. We then retrieve the k contexts with the highest overlap.\n",
        "\n",
        "**TODO:** Write the function `retrieve_overlap(question, contexts, top_k)` that takes in the question (a string) and a list of contexts (each context is a string). It should calculate the word overlap between the question and *each* context, and return a list of the *top_k* contexts with the highest overlap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "fb453256",
      "metadata": {
        "id": "fb453256"
      },
      "outputs": [],
      "source": [
        "# word overlap retriever -- write this function\n",
        "def retrieve_overlap(question, contexts, top_k=5):\n",
        "  question_tokens = set(get_tokens(question))\n",
        "\n",
        "  scores = []\n",
        "  for context in contexts:\n",
        "    context_tokens = set(get_tokens(context))\n",
        "    overlap = len (set(question_tokens) & set(context_tokens))\n",
        "    scores.append((overlap, context))\n",
        "\n",
        "  scores.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "  return [context for _, context in scores[:top_k]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "733df5da-00ba-4507-a303-3a35d1753dd6",
      "metadata": {
        "id": "733df5da-00ba-4507-a303-3a35d1753dd6"
      },
      "source": [
        "The following function runs the retriever a list of qa_items. For each qa_item it obtains the list of retrieved contexts and adds them to the qa_item."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "70979681-8b48-499e-9777-296863955856",
      "metadata": {
        "id": "70979681-8b48-499e-9777-296863955856"
      },
      "outputs": [],
      "source": [
        "def add_rag_context_overlap(qa_items, contexts, retriever, top_k=5):\n",
        "    result_items = copy.deepcopy(qa_items)\n",
        "    for inst in tqdm.tqdm(result_items, desc=\"Retrieving contexts\"):\n",
        "        question = inst['question']\n",
        "        retrieved_contexts = retriever(question, contexts, top_k)\n",
        "        inst['rag_contexts'] = retrieved_contexts\n",
        "    return result_items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "2e2d07b0-9722-4241-b568-04b04ecd900a",
      "metadata": {
        "id": "2e2d07b0-9722-4241-b568-04b04ecd900a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e89ffce2-5aa7-4b9b-b7f4-19d3156580cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving contexts: 100%|██████████| 250/250 [08:14<00:00,  1.98s/it]\n"
          ]
        }
      ],
      "source": [
        "rag_qa_pairs = add_rag_context_overlap(evaluation_benchmark['qas'], candidate_contexts, retrieve_overlap)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9fe4dd0-4849-4ec3-9e85-834d0a974e4a",
      "metadata": {
        "id": "e9fe4dd0-4849-4ec3-9e85-834d0a974e4a"
      },
      "source": [
        "It returns a copy of the qa_item list that is now annotated with the additional 'rag_contexts'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "a27e813c-c436-4bf8-80fd-e7df55b24024",
      "metadata": {
        "id": "a27e813c-c436-4bf8-80fd-e7df55b24024",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b08fa3c9-522c-4030-b66e-5a08392c52a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?',\n",
              " 'answer': 'professor Juan Pedro Toni',\n",
              " 'context': \"The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. The school's headmaster, history professor Juan Pedro Toni, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.\",\n",
              " 'rag_contexts': [\"The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. The school's headmaster, history professor Juan Pedro Toni, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.\",\n",
              "  \"Red is one of the most common colors used on national flags. The use of red has similar connotations from country to country: the blood, sacrifice, and courage of those who defended their country; the sun and the hope and warmth it brings; and the sacrifice of Christ's blood (in some historically Christian nations) are a few examples. Red is the color of the flags of several countries that once belonged to the former British Empire. The British flag bears the colors red, white, and blue; it includes the cross of Saint George, patron saint of England, and the saltire of Saint Patrick, patron saint of Ireland, both of which are red on white. The flag of the United States bears the colors of Britain, the colors of the French tricolore include red as part of the old Paris coat of arms, and other countries' flags, such as those of Australia, New Zealand, and Fiji, carry a small inset of the British flag in memory of their ties to that country. Many former colonies of Spain, such as Mexico, Colombia, Ecuador, Cuba, Puerto Rico, Peru, and Venezuela, also feature red-one of the colors of the Spanish flag-on their own banners. Red flags are also used to symbolize storms, bad water conditions, and many other dangers. Navy flags are often red and yellow. Red is prominently featured in the flag of the United States Marine Corps.\",\n",
              "  'In July 2015, Eton accidentally sent emails to 400 prospective students, offering them conditional entrance to the school in September 2017. The email was intended for nine students, but an IT glitch caused the email to be sent to 400 additional families, who didn\\'t necessarily have a place. In response, the school issued the following statement: \"This error was discovered within minutes and each family was immediately contacted to notify them that it should be disregarded and to apologise. We take this type of incident very seriously indeed and so a thorough investigation, overseen by the headmaster Tony Little and led by the tutor for admissions, is being carried out to find out exactly what went wrong and ensure it cannot happen again. Eton College offers its sincere apologies to those boys concerned and their families. We deeply regret the confusion and upset this must have caused.\"',\n",
              "  'John Evans, for whom Evanston is named, bought 379 acres (153 ha) of land along Lake Michigan in 1853, and Philo Judson developed plans for what would become the city of Evanston, Illinois. The first building, Old College, opened on November 5, 1855. To raise funds for its construction, Northwestern sold $100 \"perpetual scholarships\" entitling the purchaser and his heirs to free tuition. Another building, University Hall, was built in 1869 of the same Joliet limestone as the Chicago Water Tower, also built in 1869, one of the few buildings in the heart of Chicago to survive the Great Chicago Fire of 1871. In 1873 the Evanston College for Ladies merged with Northwestern, and Frances Willard, who later gained fame as a suffragette and as one of the founders of the Woman\\'s Christian Temperance Union (WCTU), became the school\\'s first dean of women. Willard Residential College (1938) is named in her honor. Northwestern admitted its first women students in 1869, and the first woman was graduated in 1874.',\n",
              "  'Two years later, the Emperor Valens, who favored the Arian position, in his turn exiled Athanasius. This time however, Athanasius simply left for the outskirts of Alexandria, where he stayed for only a few months before the local authorities convinced Valens to retract his order of exile. Some early reports state that Athanasius spent this period of exile at his family\\'s ancestral tomb in a Christian cemetery. It was during this period, the final exile, that he is said to have spent four months in hiding in his father\\'s tomb. (Soz., \"Hist. Eccl.\", VI, xii; Soc., \"Hist. Eccl.\", IV, xii).']}"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ],
      "source": [
        "rag_qa_pairs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cee220f-7e4d-4354-bfb0-32ef05b14492",
      "metadata": {
        "id": "5cee220f-7e4d-4354-bfb0-32ef05b14492"
      },
      "source": [
        "Before we run an end-to-end evaluation, we can check the accuracy of the word overlap retriever. In other words, for what fraction of questions is the gold context included in the top-k retrieved contexts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "4b85399d",
      "metadata": {
        "id": "4b85399d"
      },
      "outputs": [],
      "source": [
        "# evaluation metric of retriever\n",
        "def evaluate_retriever(rag_qa_pairs):\n",
        "    \"\"\"\n",
        "    Evaluates the retriever by computing the accuracy of retrieved contexts against reference contexts.\n",
        "    \"\"\"\n",
        "    correct_retrievals = 0\n",
        "    for qa_item in rag_qa_pairs:\n",
        "        if qa_item['context'] in qa_item['rag_contexts']:\n",
        "            correct_retrievals += 1\n",
        "    accuracy = correct_retrievals / len(rag_qa_pairs)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f54e6907-df91-41a7-813c-406aebba329f",
      "metadata": {
        "id": "f54e6907-df91-41a7-813c-406aebba329f"
      },
      "source": [
        "In our implementation, we got an accuracy of 0.372."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "b81b86eb-7789-4a17-87a6-c190955430e3",
      "metadata": {
        "id": "b81b86eb-7789-4a17-87a6-c190955430e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "13aba7e8-cbca-4173-cb6a-6cf366b3da56"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.52"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ],
      "source": [
        "evaluate_retriever(rag_qa_pairs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93eb3ff7-8d00-4e7b-8dbf-7b852ec1dfc8",
      "metadata": {
        "id": "93eb3ff7-8d00-4e7b-8dbf-7b852ec1dfc8"
      },
      "source": [
        "**TODO**: Write a function `rag_qa(qa_item)` that behaves like the `vanilla_qa` and `oracle_qa` functions above. Create a prompt from the question and the top-k retrieved contexts (instead of the gold context you used in `oracle_qa`). You can assume that `qa_item` already\n",
        "contains the 'rag_contexts' field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "4289b8bd-e21a-4248-bed9-efcd5d6a5e78",
      "metadata": {
        "id": "4289b8bd-e21a-4248-bed9-efcd5d6a5e78"
      },
      "outputs": [],
      "source": [
        "def rag_qa(qa_item): # Write this function\n",
        "  prompt = (\n",
        "      \"As a factual question answering system, you are going to answer question.\\n\"\n",
        "      \"Answer the question using as few words as possible.\\n\"\n",
        "      \"Do not include articles (a, an, the), punctuation, or explanations.\\n\"\n",
        "      #\"If unsure, answer with ''.\\n\\n\"\n",
        "      f\"Context: {qa_item['rag_contexts']}\"\n",
        "      f\"Question: {qa_item['question']}\\n\"\n",
        "      \"Answer:\"\n",
        "  )\n",
        "\n",
        "  output = pipe(prompt,\n",
        "            max_new_tokens=128,\n",
        "            do_sample=True, # set to False for greedy decoding below\n",
        "            pad_token_id=pipe.tokenizer.eos_token_id)\n",
        "\n",
        "  answer = output[0]['generated_text'][len(prompt):].strip()\n",
        "  return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b6d1181-df6b-4f18-b038-c83ad3d63ad7",
      "metadata": {
        "id": "8b6d1181-df6b-4f18-b038-c83ad3d63ad7"
      },
      "source": [
        "**TODO**: Like you did for the vanilla and oracle qa system, evaluate the `rag_qa` function and display the results. In our implementation, we got an exact match of 19.6%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "2f85125b-a462-4355-92b9-c930e1e4cb10",
      "metadata": {
        "id": "2f85125b-a462-4355-92b9-c930e1e4cb10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "946ca1eb-b81f-46ad-8dd5-feabf481c41a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating QA instances: 100%|██████████| 250/250 [00:51<00:00,  4.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Evaluation Results:\n",
            "Exact Match: 30.00%\n",
            "F1 Score: 39.24%\n",
            "ROUGE2 F1: 17.95%\n",
            "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
            "Gold Answer: professor Juan Pedro Toni\n",
            "Predicted Answer: Juan Pedro Toni\n",
            "Exact Match: 0, F1 Score: 0.8571428571428571\n",
            "ROUGE-2 F1-score: 0.8\n",
            "----------------------------------------\n",
            "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
            "Gold Answer: about six to four\n",
            "Predicted Answer: 10.9%\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
            "Gold Answer: 1890s\n",
            "Predicted Answer: 1890s\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did devolution in the UK begin?\n",
            "Gold Answer: 1914\n",
            "Predicted Answer: 1908\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: Treating the mitrailleuse like what rendered it far less effective\n",
            "Gold Answer: artillery\n",
            "Predicted Answer: an ambush\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "rag_overlap_eval = evaluate_qa(rag_qa, rag_qa_pairs)\n",
        "present_results(rag_overlap_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebca56df-f99d-4cf6-b877-cde203b102dc",
      "metadata": {
        "id": "ebca56df-f99d-4cf6-b877-cde203b102dc"
      },
      "source": [
        "## Part 5 - Retrieval-Augmented Question Answering - Dense Retrieval\n",
        "\n",
        "In this step, we will try to will encode each context and questions using BERT. We will then retrieve the k contexts whose embeddings have the highest cosine similarity to the question embedding.\n",
        "\n",
        "### 5.1 Creating Embeddings for Contexts and Questions\n",
        "\n",
        "Here is an example for how to use BERT to encode a sentence. Instead of using the CLS embeddings (as discussed in class) we will pool together the token representations at the last layer by averaging. The resulting representation is a (1,768) tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "1c3d6d9d-aaac-4708-8c40-ee8f06046275",
      "metadata": {
        "id": "1c3d6d9d-aaac-4708-8c40-ee8f06046275"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\"\n",
        "from transformers import BertTokenizer, BertModel # If you run into memory issues, you\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "\n",
        "inputs = tokenizer(\"This is a sample sentence.\", return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    hidden_states = outputs.last_hidden_state\n",
        "    embedding = torch.mean(hidden_states, dim=1)  # (batch_size=1, embedding size =768)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "18093c30-2133-4b0c-a5be-15534f6f373f",
      "metadata": {
        "id": "18093c30-2133-4b0c-a5be-15534f6f373f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1728c01f-4160-4091-9e9b-f1abaccc3d49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ],
      "source": [
        "embedding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5e7db1f-025e-4c70-a4c8-03d70bf433de",
      "metadata": {
        "id": "f5e7db1f-025e-4c70-a4c8-03d70bf433de"
      },
      "source": [
        "**TODO**: Write code to encode each candidate context. Stack the embeddings together into a single (19035, 768) pytorch tensor that we can save to disk and reload as needed (see above for how to access the candidate contexts). On some lower-resource systems you may have trouble instantiating both BERT and OLMo2 at the same time. Storing the encoded representations allows you to run just OLMo for the QA part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "f7c617ff-8a22-451d-8144-b29d959e7bcf",
      "metadata": {
        "id": "f7c617ff-8a22-451d-8144-b29d959e7bcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ba8c3c83-54e5-4677-e034-cffca7efa196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding candidate contexts: 100%|██████████| 595/595 [01:39<00:00,  5.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final shape: torch.Size([19035, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "max_length = 256\n",
        "\n",
        "embedding_list = []\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in tqdm.tqdm(\n",
        "        range(0, len(candidate_contexts), batch_size),\n",
        "        desc=\"Encoding candidate contexts\"\n",
        "    ):\n",
        "        batch_contexts = candidate_contexts[i:i + batch_size]\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            batch_contexts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length\n",
        "        ).to(device)\n",
        "\n",
        "        outputs = model(**inputs)  # (B, L, 768)\n",
        "\n",
        "        # ---- mean pooling ----\n",
        "        attention_mask = inputs[\"attention_mask\"].unsqueeze(-1)  # (B, L, 1)\n",
        "        summed = (outputs.last_hidden_state * attention_mask).sum(dim=1)\n",
        "        counts = attention_mask.sum(dim=1).clamp(min=1e-9)\n",
        "        batch_embeddings = summed / counts                       # (B, 768)\n",
        "\n",
        "        embedding_list.append(batch_embeddings.cpu())\n",
        "\n",
        "# Stack into a single tensor\n",
        "context_embeddings = torch.cat(embedding_list, dim=0)\n",
        "print(\"Final shape:\", context_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "5b19f021-e9fc-43fa-a54d-b49f268c506e",
      "metadata": {
        "id": "5b19f021-e9fc-43fa-a54d-b49f268c506e"
      },
      "outputs": [],
      "source": [
        "torch.save(context_embeddings, \"context_embeddings.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79dc61f3-00c8-493a-812b-0eed87969197",
      "metadata": {
        "id": "79dc61f3-00c8-493a-812b-0eed87969197"
      },
      "source": [
        "**TODO**: Similarly encode each question and stack the embeddings together into a single (250, 768) pytorch tensor that we can save to disk and reload as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "id": "53f85eda-0b12-4934-bd57-1ffadbfd73cb",
      "metadata": {
        "id": "53f85eda-0b12-4934-bd57-1ffadbfd73cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "361545cf-0822-4c2a-bbe9-14c335c76f8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding questions: 100%|██████████| 8/8 [00:00<00:00, 45.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final question embedding shape: torch.Size([250, 768])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "max_length = 64\n",
        "\n",
        "question_texts = [qa[\"question\"] for qa in evaluation_benchmark[\"qas\"]]\n",
        "\n",
        "question_embedding_list = []\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in tqdm.tqdm(\n",
        "        range(0, len(question_texts), batch_size),\n",
        "        desc=\"Encoding questions\"\n",
        "    ):\n",
        "        batch_questions = question_texts[i:i + batch_size]\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            batch_questions,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_length\n",
        "        ).to(device)\n",
        "\n",
        "        outputs = model(**inputs)  # (B, L, 768)\n",
        "\n",
        "        # ---- mean pooling ----\n",
        "        attention_mask = inputs[\"attention_mask\"].unsqueeze(-1)  # (B, L, 1)\n",
        "        summed = (outputs.last_hidden_state * attention_mask).sum(dim=1)\n",
        "        counts = attention_mask.sum(dim=1).clamp(min=1e-9)\n",
        "        batch_embeddings = summed / counts                       # (B, 768)\n",
        "\n",
        "        question_embedding_list.append(batch_embeddings.cpu())\n",
        "\n",
        "# Stack into (250, 768)\n",
        "question_embeddings = torch.cat(question_embedding_list, dim=0)\n",
        "\n",
        "print(\"Final question embedding shape:\", question_embeddings.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "id": "1f1f0207-180d-467f-8a79-0e0e6a45f215",
      "metadata": {
        "id": "1f1f0207-180d-467f-8a79-0e0e6a45f215"
      },
      "outputs": [],
      "source": [
        "torch.save(question_embeddings, \"question_embeddings.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04a95403-40b2-4bb7-b36e-7a8236399a46",
      "metadata": {
        "id": "04a95403-40b2-4bb7-b36e-7a8236399a46"
      },
      "source": [
        "### 5.2 Similarity Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "id": "65ae157a-c526-4cc7-9d6c-95cd291398aa",
      "metadata": {
        "id": "65ae157a-c526-4cc7-9d6c-95cd291398aa"
      },
      "outputs": [],
      "source": [
        "context_embeddings = torch.load(\"context_embeddings.pt\")\n",
        "question_embeddings = torch.load(\"question_embeddings.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a031d32f-4a0d-42e2-ae87-b397e7a27ab7",
      "metadata": {
        "id": "a031d32f-4a0d-42e2-ae87-b397e7a27ab7"
      },
      "source": [
        "**TODO**: Write a function `retrieve_cosine(question_embedding, contexts, context_embeddings)` that takes in the embedding for a single question (a [1,768] tensor), a list of contexts (each is a string), and the context embedding tensor [19035,768].\n",
        "Note that the indices of the context list and the rows of the context_embeddings tensor line up. i.e. `context_embeddings[0]` is the embedding for `contexts[0]`, etc.\n",
        "You can use `torch.nn.functional.cosine_similarity` (or `F.cosine_similarity` since we imported `torch.nn.functional` as `F`, which is conventional) to calculate the similarities efficiently. You may also ant to look at `torch.topk`, but other solutions are possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "id": "40603f69-f627-4b68-9ef2-4019c78b5b39",
      "metadata": {
        "id": "40603f69-f627-4b68-9ef2-4019c78b5b39"
      },
      "outputs": [],
      "source": [
        "def retrieve_cosine(question_emb, contexts, context_embeddings, top_k=5):\n",
        "    \"\"\"\n",
        "    question_emb: Tensor of shape (1, 768)\n",
        "    contexts: list of context strings (length 19035)\n",
        "    context_embeddings: Tensor of shape (19035, 768)\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure shapes are compatible\n",
        "    if question_emb.dim() == 2:\n",
        "        question_emb = question_emb.squeeze(0)  # (768,)\n",
        "\n",
        "    # Compute cosine similarities: (19035,)\n",
        "    similarities = F.cosine_similarity(\n",
        "        context_embeddings,          # (19035, 768)\n",
        "        question_emb.unsqueeze(0),   # (1, 768) -> broadcast\n",
        "        dim=1\n",
        "    )\n",
        "\n",
        "    # indices of top-k most similar contexts\n",
        "    topk_scores, topk_indices = torch.topk(similarities, k=top_k)\n",
        "\n",
        "    # Return contexts\n",
        "    return [contexts[i] for i in topk_indices.tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "id": "a56ef08f-39a3-4a5b-94a7-e70d139541e6",
      "metadata": {
        "id": "a56ef08f-39a3-4a5b-94a7-e70d139541e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a002ebb1-d3b9-466c-f4b1-d64796a39028"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. The school's headmaster, history professor Juan Pedro Toni, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.\",\n",
              " 'The National Maritime College of Ireland is also located in Cork and is the only college in Ireland in which Nautical Studies and Marine Engineering can be undertaken. CIT also incorporates the Cork School of Music and Crawford College of Art and Design as constituent schools. The Cork College of Commerce is the largest post-Leaving Certificate college in Ireland and is also the biggest provider of Vocational Preparation and Training courses in the country.[citation needed] Other 3rd level institutions include Griffith College Cork, a private institution, and various other colleges.',\n",
              " 'The University of St Mark & St John (known as \"Marjon\" or \"Marjons\") specialises in teacher training, and offers training across the country and abroad.',\n",
              " \"Eton College has links with some private schools in India today, maintained from the days of the British Raj, such as The Doon School and Mayo College. Eton College is also a member of the G20 Schools Group, a collection of college preparatory boarding schools from around the world, including Turkey's Robert College, the United States' Phillips Academy and Phillips Exeter Academy, Australia's Scotch College, Melbourne Grammar School and Launceston Church Grammar School, Singapore's Raffles Institution, and Switzerland's International School of Geneva. Eton has recently fostered[when?] a relationship with the Roxbury Latin School, a traditional all-boys private school in Boston, USA. Former Eton headmaster and provost Sir Eric Anderson shares a close friendship with Roxbury Latin Headmaster emeritus F. Washington Jarvis; Anderson has visited Roxbury Latin on numerous occasions, while Jarvis briefly taught theology at Eton after retiring from his headmaster post at Roxbury Latin. The headmasters' close friendship spawned the Hennessy Scholarship, an annual prize established in 2005 and awarded to a graduating RL senior for a year of study at Eton. Hennessy Scholars generally reside in Wotton house.\",\n",
              " \"Detroit is served by various private schools, as well as parochial Roman Catholic schools operated by the Archdiocese of Detroit. As of 2013[update] there are four Catholic grade schools and three Catholic high schools in the City of Detroit, with all of them in the city's west side. The Archdiocese of Detroit lists a number of primary and secondary schools in the metro area as Catholic education has emigrated to the suburbs. Of the three Catholic high schools in the city, two are operated by the Society of Jesus and the third is co-sponsored by the Sisters, Servants of the Immaculate Heart of Mary and the Congregation of St. Basil.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ],
      "source": [
        "retrieve_cosine(question_embeddings[0], candidate_contexts, context_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dba5a9c-b04c-421e-92f9-7b4fcabc0946",
      "metadata": {
        "id": "8dba5a9c-b04c-421e-92f9-7b4fcabc0946"
      },
      "source": [
        "**TODO**: Write a new version of the add_rag_context function we provided above. This function should now additionally take the question embeddings and context embeddings as parameters, run the retrieval for each question (using the retrieve_cosine function above) and populate a new list of qa_items, include the selected 'rag_contexts'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "id": "7ea939c2-cc28-43b9-a1b2-547ce8042e99",
      "metadata": {
        "id": "7ea939c2-cc28-43b9-a1b2-547ce8042e99"
      },
      "outputs": [],
      "source": [
        "def add_rag_context_dense(qa_items, contexts, retriever, question_embeddings, context_embeddings, top_k=5):\n",
        "    \"\"\"\n",
        "    qa_items: list of QA dicts (length 250)\n",
        "    contexts: list of all candidate contexts (length 19035)\n",
        "    retriever: retrieve_cosine function\n",
        "    question_embeddings: Tensor (250, 768)\n",
        "    context_embeddings: Tensor (19035, 768)\n",
        "    \"\"\"\n",
        "\n",
        "    result_items = copy.deepcopy(qa_items)\n",
        "\n",
        "    for i, qa_item in tqdm.tqdm(\n",
        "        enumerate(result_items),\n",
        "        total=len(result_items),\n",
        "        desc=\"Retrieving dense contexts\"\n",
        "    ):\n",
        "        question_emb = question_embeddings[i].unsqueeze(0)  # (1, 768)\n",
        "\n",
        "        retrieved_contexts = retriever(\n",
        "            question_emb,\n",
        "            contexts,\n",
        "            context_embeddings,\n",
        "            top_k=top_k\n",
        "        )\n",
        "\n",
        "        qa_item[\"rag_contexts\"] = retrieved_contexts\n",
        "\n",
        "    return result_items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "id": "cb2a6add-eec1-40c1-a0b4-8368661ca6ac",
      "metadata": {
        "id": "cb2a6add-eec1-40c1-a0b4-8368661ca6ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a301f643-4b0e-4153-a192-633b70fe0584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving dense contexts: 100%|██████████| 250/250 [00:07<00:00, 31.62it/s]\n"
          ]
        }
      ],
      "source": [
        "rag_qa_items = add_rag_context_dense(evaluation_benchmark['qas'], candidate_contexts, retrieve_cosine, question_embeddings, context_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "id": "cd9708cb-961d-4aaa-9d6b-5c62895adf4e",
      "metadata": {
        "id": "cd9708cb-961d-4aaa-9d6b-5c62895adf4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a1f1c82a-0657-4cd2-f0c8-f497d708d1b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?',\n",
              " 'answer': 'professor Juan Pedro Toni',\n",
              " 'context': \"The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. The school's headmaster, history professor Juan Pedro Toni, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.\",\n",
              " 'rag_contexts': [\"The Christian Brothers of Ireland Stella Maris College is a private, co-educational, not-for-profit Catholic school located in the wealthy residential southeastern neighbourhood of Carrasco. Established in 1955, it is regarded as one of the best high schools in the country, blending a rigorous curriculum with strong extracurricular activities. The school's headmaster, history professor Juan Pedro Toni, is a member of the Stella Maris Board of Governors and the school is a member of the International Baccalaureate Organization (IBO). Its long list of distinguished former pupils includes economists, engineers, architects, lawyers, politicians and even F1 champions. The school has also played an important part in the development of rugby union in Uruguay, with the creation of Old Christians Club, the school's alumni club.\",\n",
              "  'The National Maritime College of Ireland is also located in Cork and is the only college in Ireland in which Nautical Studies and Marine Engineering can be undertaken. CIT also incorporates the Cork School of Music and Crawford College of Art and Design as constituent schools. The Cork College of Commerce is the largest post-Leaving Certificate college in Ireland and is also the biggest provider of Vocational Preparation and Training courses in the country.[citation needed] Other 3rd level institutions include Griffith College Cork, a private institution, and various other colleges.',\n",
              "  'The University of St Mark & St John (known as \"Marjon\" or \"Marjons\") specialises in teacher training, and offers training across the country and abroad.',\n",
              "  \"Eton College has links with some private schools in India today, maintained from the days of the British Raj, such as The Doon School and Mayo College. Eton College is also a member of the G20 Schools Group, a collection of college preparatory boarding schools from around the world, including Turkey's Robert College, the United States' Phillips Academy and Phillips Exeter Academy, Australia's Scotch College, Melbourne Grammar School and Launceston Church Grammar School, Singapore's Raffles Institution, and Switzerland's International School of Geneva. Eton has recently fostered[when?] a relationship with the Roxbury Latin School, a traditional all-boys private school in Boston, USA. Former Eton headmaster and provost Sir Eric Anderson shares a close friendship with Roxbury Latin Headmaster emeritus F. Washington Jarvis; Anderson has visited Roxbury Latin on numerous occasions, while Jarvis briefly taught theology at Eton after retiring from his headmaster post at Roxbury Latin. The headmasters' close friendship spawned the Hennessy Scholarship, an annual prize established in 2005 and awarded to a graduating RL senior for a year of study at Eton. Hennessy Scholars generally reside in Wotton house.\",\n",
              "  \"Detroit is served by various private schools, as well as parochial Roman Catholic schools operated by the Archdiocese of Detroit. As of 2013[update] there are four Catholic grade schools and three Catholic high schools in the City of Detroit, with all of them in the city's west side. The Archdiocese of Detroit lists a number of primary and secondary schools in the metro area as Catholic education has emigrated to the suburbs. Of the three Catholic high schools in the city, two are operated by the Society of Jesus and the third is co-sponsored by the Sisters, Servants of the Immaculate Heart of Mary and the Congregation of St. Basil.\"]}"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ],
      "source": [
        "rag_qa_items[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "922c0005-f4fd-4094-8e14-7f000bdc4c28",
      "metadata": {
        "id": "922c0005-f4fd-4094-8e14-7f000bdc4c28"
      },
      "source": [
        "Run the `evaluate_retriever` function on the new qa_items. In our experiments, we got an accuracy of about 0.4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "id": "f65ef204-69f9-47a4-8670-03aa6bea5277",
      "metadata": {
        "id": "f65ef204-69f9-47a4-8670-03aa6bea5277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a88ea332-a80c-479f-b65a-a718b08d7ffd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.412"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ],
      "source": [
        "evaluate_retriever(rag_qa_items)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7282ac18-789e-4f8b-9b78-6f64c671d736",
      "metadata": {
        "id": "7282ac18-789e-4f8b-9b78-6f64c671d736"
      },
      "source": [
        "Then, evaluate the rag_qa approach using the revised rag_qa_items. You should get an Exact match better than 20%.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "id": "4cccddd0-731e-4807-82e8-b73ffa59838e",
      "metadata": {
        "id": "4cccddd0-731e-4807-82e8-b73ffa59838e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8aa17dd9-0060-4d51-8257-116bcb66c4a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating QA instances: 100%|██████████| 250/250 [00:35<00:00,  6.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Evaluation Results:\n",
            "Exact Match: 24.00%\n",
            "F1 Score: 32.98%\n",
            "ROUGE2 F1: 13.64%\n",
            "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
            "Gold Answer: professor Juan Pedro Toni\n",
            "Predicted Answer: Juan Pedro Toni\n",
            "Exact Match: 0, F1 Score: 0.8571428571428571\n",
            "ROUGE-2 F1-score: 0.8\n",
            "----------------------------------------\n",
            "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
            "Gold Answer: about six to four\n",
            "Predicted Answer: 1\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
            "Gold Answer: 1890s\n",
            "Predicted Answer: 1896\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did devolution in the UK begin?\n",
            "Gold Answer: 1914\n",
            "Predicted Answer: 1914\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: Treating the mitrailleuse like what rendered it far less effective\n",
            "Gold Answer: artillery\n",
            "Predicted Answer: French\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "result = evaluate_qa(rag_qa, rag_qa_items)\n",
        "present_results(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5373f0ec",
      "metadata": {
        "id": "5373f0ec"
      },
      "source": [
        "## Part 6 - Experiments\n",
        "\n",
        "**TODO** For the overlap and dense retrievers (from part 5 and 6), what happens when you change the number of retrieved contexts? Present a table of results for k=1, k=5 (already done), k=10, and k=20.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ks = [1, 5, 10, 20]\n",
        "\n",
        "overlap_results = {}\n",
        "\n",
        "for k in ks:\n",
        "    print(f\"\\nOverlap RAG k={k}\")\n",
        "    rag_pairs = add_rag_context_overlap(\n",
        "        evaluation_benchmark[\"qas\"],\n",
        "        candidate_contexts,\n",
        "        retrieve_overlap,\n",
        "        top_k=k\n",
        "    )\n",
        "    eval_results = evaluate_qa(rag_qa, rag_pairs)\n",
        "    present_results(eval_results, f\"Overlap RAG (k={k})\")\n",
        "\n",
        "for k in ks:\n",
        "    print(f\"\\nDense RAG k={k}\")\n",
        "    rag_pairs = add_rag_context_dense(\n",
        "        evaluation_benchmark[\"qas\"],\n",
        "        candidate_contexts,\n",
        "        retrieve_cosine,\n",
        "        question_embeddings,\n",
        "        context_embeddings,\n",
        "        top_k=k\n",
        "    )\n",
        "    eval_results = evaluate_qa(rag_qa, rag_pairs)\n",
        "    present_results(eval_results, f\"Dense RAG (k={k})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TmVCetokJasp",
        "outputId": "84135aa0-0a40-4fcd-c36e-013f7ae627ba"
      },
      "id": "TmVCetokJasp",
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Overlap RAG k=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving contexts: 100%|██████████| 250/250 [08:19<00:00,  2.00s/it]\n",
            "Evaluating QA instances: 100%|██████████| 250/250 [00:28<00:00,  8.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overlap RAG (k=1) Evaluation Results:\n",
            "Exact Match: 21.20%\n",
            "F1 Score: 28.95%\n",
            "ROUGE2 F1: 10.39%\n",
            "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
            "Gold Answer: professor Juan Pedro Toni\n",
            "Predicted Answer: Juan Pedro Toni\n",
            "Exact Match: 0, F1 Score: 0.8571428571428571\n",
            "ROUGE-2 F1-score: 0.8\n",
            "----------------------------------------\n",
            "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
            "Gold Answer: about six to four\n",
            "Predicted Answer: 10.9%\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
            "Gold Answer: 1890s\n",
            "Predicted Answer: 1890s\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did devolution in the UK begin?\n",
            "Gold Answer: 1914\n",
            "Predicted Answer: 500 BC\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: Treating the mitrailleuse like what rendered it far less effective\n",
            "Gold Answer: artillery\n",
            "Predicted Answer: no\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "\n",
            "Overlap RAG k=5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving contexts: 100%|██████████| 250/250 [08:19<00:00,  2.00s/it]\n",
            "Evaluating QA instances: 100%|██████████| 250/250 [00:44<00:00,  5.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overlap RAG (k=5) Evaluation Results:\n",
            "Exact Match: 32.40%\n",
            "F1 Score: 40.29%\n",
            "ROUGE2 F1: 18.25%\n",
            "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
            "Gold Answer: professor Juan Pedro Toni\n",
            "Predicted Answer: Juan Pedro Toni\n",
            "Exact Match: 0, F1 Score: 0.8571428571428571\n",
            "ROUGE-2 F1-score: 0.8\n",
            "----------------------------------------\n",
            "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
            "Gold Answer: about six to four\n",
            "Predicted Answer: 10.9%\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
            "Gold Answer: 1890s\n",
            "Predicted Answer: 1890s\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did devolution in the UK begin?\n",
            "Gold Answer: 1914\n",
            "Predicted Answer: 1908\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: Treating the mitrailleuse like what rendered it far less effective\n",
            "Gold Answer: artillery\n",
            "Predicted Answer: no\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "\n",
            "Overlap RAG k=10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving contexts: 100%|██████████| 250/250 [08:21<00:00,  2.01s/it]\n",
            "Evaluating QA instances:  28%|██▊       | 70/250 [00:14<00:33,  5.38it/s]This is a friendly reminder - the current text generation call has exceeded the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
            "Evaluating QA instances: 100%|██████████| 250/250 [01:00<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overlap RAG (k=10) Evaluation Results:\n",
            "Exact Match: 27.20%\n",
            "F1 Score: 38.02%\n",
            "ROUGE2 F1: 18.05%\n",
            "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
            "Gold Answer: professor Juan Pedro Toni\n",
            "Predicted Answer: Juan Pedro Toni\n",
            "Exact Match: 0, F1 Score: 0.8571428571428571\n",
            "ROUGE-2 F1-score: 0.8\n",
            "----------------------------------------\n",
            "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
            "Gold Answer: about six to four\n",
            "Predicted Answer: 3.4%\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
            "Gold Answer: 1890s\n",
            "Predicted Answer: 1890s\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did devolution in the UK begin?\n",
            "Gold Answer: 1914\n",
            "Predicted Answer: 1973\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: Treating the mitrailleuse like what rendered it far less effective\n",
            "Gold Answer: artillery\n",
            "Predicted Answer: less reflective and thus less effective\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "\n",
            "Overlap RAG k=20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving contexts: 100%|██████████| 250/250 [08:20<00:00,  2.00s/it]\n",
            "Evaluating QA instances: 100%|██████████| 250/250 [07:42<00:00,  1.85s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overlap RAG (k=20) Evaluation Results:\n",
            "Exact Match: 10.00%\n",
            "F1 Score: 15.23%\n",
            "ROUGE2 F1: 7.18%\n",
            "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
            "Gold Answer: professor Juan Pedro Toni\n",
            "Predicted Answer: John Evans\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
            "Gold Answer: about six to four\n",
            "Predicted Answer: 3.4\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
            "Gold Answer: 1890s\n",
            "Predicted Answer: 1890\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did devolution in the UK begin?\n",
            "Gold Answer: 1914\n",
            "Predicted Answer: The UK began in the late 18th century.\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: Treating the mitrailleuse like what rendered it far less effective\n",
            "Gold Answer: artillery\n",
            "Predicted Answer: 200's that one's, a short's enough, the former: 1, a plus 36th that one instance, with the one specific examples, one instance, the last. A, it the first time, it's version, it's an example. The first time's that many times whereof \"O's code's that many times: 1. 700, 500 of 1510. It is not's that's named. 500 of 700, from the times, once in the times, and no longer than that, and finally the latest. The last. The last as, there\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "\n",
            "Dense RAG k=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving dense contexts: 100%|██████████| 250/250 [00:08<00:00, 31.19it/s]\n",
            "Evaluating QA instances: 100%|██████████| 250/250 [00:29<00:00,  8.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense RAG (k=1) Evaluation Results:\n",
            "Exact Match: 14.40%\n",
            "F1 Score: 19.21%\n",
            "ROUGE2 F1: 5.60%\n",
            "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
            "Gold Answer: professor Juan Pedro Toni\n",
            "Predicted Answer: Juan Pedro Toni\n",
            "Exact Match: 0, F1 Score: 0.8571428571428571\n",
            "ROUGE-2 F1-score: 0.8\n",
            "----------------------------------------\n",
            "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
            "Gold Answer: about six to four\n",
            "Predicted Answer: 1\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
            "Gold Answer: 1890s\n",
            "Predicted Answer: 1903\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did devolution in the UK begin?\n",
            "Gold Answer: 1914\n",
            "Predicted Answer: 1914\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: Treating the mitrailleuse like what rendered it far less effective\n",
            "Gold Answer: artillery\n",
            "Predicted Answer: mounted on an artillery gun carriage and grouped in batteries in a similar fashion to cannon.\n",
            "Exact Match: 0, F1 Score: 0.13333333333333333\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "\n",
            "Dense RAG k=5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving dense contexts: 100%|██████████| 250/250 [00:08<00:00, 30.98it/s]\n",
            "Evaluating QA instances: 100%|██████████| 250/250 [00:32<00:00,  7.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense RAG (k=5) Evaluation Results:\n",
            "Exact Match: 24.40%\n",
            "F1 Score: 32.92%\n",
            "ROUGE2 F1: 13.43%\n",
            "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
            "Gold Answer: professor Juan Pedro Toni\n",
            "Predicted Answer: Juan Pedro Toni\n",
            "Exact Match: 0, F1 Score: 0.8571428571428571\n",
            "ROUGE-2 F1-score: 0.8\n",
            "----------------------------------------\n",
            "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
            "Gold Answer: about six to four\n",
            "Predicted Answer: 1\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
            "Gold Answer: 1890s\n",
            "Predicted Answer: 1896\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did devolution in the UK begin?\n",
            "Gold Answer: 1914\n",
            "Predicted Answer: 1914\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: Treating the mitrailleuse like what rendered it far less effective\n",
            "Gold Answer: artillery\n",
            "Predicted Answer: artillery\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "\n",
            "Dense RAG k=10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving dense contexts: 100%|██████████| 250/250 [00:08<00:00, 31.09it/s]\n",
            "Evaluating QA instances: 100%|██████████| 250/250 [00:42<00:00,  5.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense RAG (k=10) Evaluation Results:\n",
            "Exact Match: 28.40%\n",
            "F1 Score: 36.62%\n",
            "ROUGE2 F1: 17.58%\n",
            "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
            "Gold Answer: professor Juan Pedro Toni\n",
            "Predicted Answer: Juan Pedro Toni\n",
            "Exact Match: 0, F1 Score: 0.8571428571428571\n",
            "ROUGE-2 F1-score: 0.8\n",
            "----------------------------------------\n",
            "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
            "Gold Answer: about six to four\n",
            "Predicted Answer: 6 to 4\n",
            "Exact Match: 0, F1 Score: 0.28571428571428575\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
            "Gold Answer: 1890s\n",
            "Predicted Answer: 1896\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did devolution in the UK begin?\n",
            "Gold Answer: 1914\n",
            "Predicted Answer: 1914\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: Treating the mitrailleuse like what rendered it far less effective\n",
            "Gold Answer: artillery\n",
            "Predicted Answer: French\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "\n",
            "Dense RAG k=20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving dense contexts: 100%|██████████| 250/250 [00:08<00:00, 30.81it/s]\n",
            "Evaluating QA instances: 100%|██████████| 250/250 [01:25<00:00,  2.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense RAG (k=20) Evaluation Results:\n",
            "Exact Match: 20.00%\n",
            "F1 Score: 31.87%\n",
            "ROUGE2 F1: 13.54%\n",
            "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
            "Gold Answer: professor Juan Pedro Toni\n",
            "Predicted Answer: Juan Pedro Toni\n",
            "Exact Match: 0, F1 Score: 0.8571428571428571\n",
            "ROUGE-2 F1-score: 0.8\n",
            "----------------------------------------\n",
            "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
            "Gold Answer: about six to four\n",
            "Predicted Answer: 6 to 4\n",
            "Exact Match: 0, F1 Score: 0.28571428571428575\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
            "Gold Answer: 1890s\n",
            "Predicted Answer: 1896\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did devolution in the UK begin?\n",
            "Gold Answer: 1914\n",
            "Predicted Answer: 1914\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: Treating the mitrailleuse like what rendered it far less effective\n",
            "Gold Answer: artillery\n",
            "Predicted Answer: French\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = [\n",
        "    [\"Overlap\", 1, 21.20, 28.95, 10.39],\n",
        "    [\"Overlap\", 5, 32.40, 40.29, 18.25],\n",
        "    [\"Overlap\", 10, 27.20, 38.02, 18.05],\n",
        "    [\"Overlap\", 20, 10.00, 15.23, 7.18],\n",
        "    [\"Dense\", 1, 14.40, 19.21, 5.60],\n",
        "    [\"Dense\", 5, 24.40, 32.92, 13.43],\n",
        "    [\"Dense\", 10, 28.40, 36.62, 17.58],\n",
        "    [\"Dense\", 20, 20.00, 31.87, 13.54],\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    data,\n",
        "    columns=[\"Retriever\", \"k\", \"Exact Match (%)\", \"F1 Score (%)\", \"ROUGE-2 F1 (%)\"]\n",
        ")\n",
        "\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "2-lx2ue9bYnv",
        "outputId": "af98aaae-d6e8-4e9b-df22-3ef27789aa17"
      },
      "id": "2-lx2ue9bYnv",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Retriever   k  Exact Match (%)  F1 Score (%)  ROUGE-2 F1 (%)\n",
              "0   Overlap   1             21.2         28.95           10.39\n",
              "1   Overlap   5             32.4         40.29           18.25\n",
              "2   Overlap  10             27.2         38.02           18.05\n",
              "3   Overlap  20             10.0         15.23            7.18\n",
              "4     Dense   1             14.4         19.21            5.60\n",
              "5     Dense   5             24.4         32.92           13.43\n",
              "6     Dense  10             28.4         36.62           17.58\n",
              "7     Dense  20             20.0         31.87           13.54"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-08623120-65a6-4347-b591-d4728474b927\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Retriever</th>\n",
              "      <th>k</th>\n",
              "      <th>Exact Match (%)</th>\n",
              "      <th>F1 Score (%)</th>\n",
              "      <th>ROUGE-2 F1 (%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Overlap</td>\n",
              "      <td>1</td>\n",
              "      <td>21.2</td>\n",
              "      <td>28.95</td>\n",
              "      <td>10.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Overlap</td>\n",
              "      <td>5</td>\n",
              "      <td>32.4</td>\n",
              "      <td>40.29</td>\n",
              "      <td>18.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Overlap</td>\n",
              "      <td>10</td>\n",
              "      <td>27.2</td>\n",
              "      <td>38.02</td>\n",
              "      <td>18.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Overlap</td>\n",
              "      <td>20</td>\n",
              "      <td>10.0</td>\n",
              "      <td>15.23</td>\n",
              "      <td>7.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Dense</td>\n",
              "      <td>1</td>\n",
              "      <td>14.4</td>\n",
              "      <td>19.21</td>\n",
              "      <td>5.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Dense</td>\n",
              "      <td>5</td>\n",
              "      <td>24.4</td>\n",
              "      <td>32.92</td>\n",
              "      <td>13.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Dense</td>\n",
              "      <td>10</td>\n",
              "      <td>28.4</td>\n",
              "      <td>36.62</td>\n",
              "      <td>17.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Dense</td>\n",
              "      <td>20</td>\n",
              "      <td>20.0</td>\n",
              "      <td>31.87</td>\n",
              "      <td>13.54</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-08623120-65a6-4347-b591-d4728474b927')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-08623120-65a6-4347-b591-d4728474b927 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-08623120-65a6-4347-b591-d4728474b927');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d8845c5f-fb3a-4c8d-b866-b24e715e5fab\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d8845c5f-fb3a-4c8d-b866-b24e715e5fab')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d8845c5f-fb3a-4c8d-b866-b24e715e5fab button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_2c10db7a-7251-4309-be81-3b4686478846\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2c10db7a-7251-4309-be81-3b4686478846 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Retriever\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Dense\",\n          \"Overlap\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"k\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7,\n        \"min\": 1,\n        \"max\": 20,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          5,\n          20\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Exact Match (%)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.443309555759262,\n        \"min\": 10.0,\n        \"max\": 32.4,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          32.4,\n          24.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1 Score (%)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.948445096376084,\n        \"min\": 15.23,\n        \"max\": 40.29,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          40.29,\n          32.92\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ROUGE-2 F1 (%)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.927508063340507,\n        \"min\": 5.6,\n        \"max\": 18.25,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          18.25,\n          13.43\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Y_L401DJUr-K"
      },
      "id": "Y_L401DJUr-K"
    },
    {
      "cell_type": "markdown",
      "id": "69a61d22-65b5-4be0-b607-9afb9be97623",
      "metadata": {
        "id": "69a61d22-65b5-4be0-b607-9afb9be97623"
      },
      "source": [
        "  ## Part 7 -Improving the QA System\n",
        "\n",
        "  **TODO**\n",
        "  In this part, we ask you to come up with one interesting or novel idea for improving the QA system. Your system does *not* have to outperform the models from part 4 or 5, but for full credit you should implement at least one new idea, beyond just changing parameters. You can either work on better retrieval or better QA/LLM performance. Show the full code for the necessary steps and evaluation results.\n",
        "\n",
        "  Ideas for improving the retriever include: improved word overlap (better tokenization/ text normalization, using TF-IDF, ...), or choosing a different approach or different model (other than BERT) for calculating context and question embeddings.\n",
        "\n",
        "  For the LLM, you could try a different transformer model, including text-to-text models (e.g. T5).                                                                                                           \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# build TF-IDF index over candidate contexts\n",
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    tokenizer=get_tokens,\n",
        "    lowercase=False,\n",
        "    preprocessor=None,\n",
        "    token_pattern=None\n",
        ")\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(candidate_contexts)\n",
        "\n",
        "def retrieve_tfidf(question, contexts, top_k=5):\n",
        "    q_vec = tfidf_vectorizer.transform([question])  # (1, vocab)\n",
        "\n",
        "    # cosine similarity via dot product (TF-IDF vectors are L2-normalized)\n",
        "    scores = (tfidf_matrix @ q_vec.T).toarray().squeeze()\n",
        "\n",
        "    top_idx = np.argsort(scores)[::-1][:top_k]\n",
        "    return [contexts[i] for i in top_idx]\n",
        "\n",
        "#top k 8 had the highest result\n",
        "rag_qa_pairs_tfidf = add_rag_context_overlap(evaluation_benchmark['qas'], candidate_contexts, retrieve_tfidf, top_k=8)\n",
        "\n",
        "evaluate_retriever(rag_qa_pairs_tfidf)\n",
        "rag_tfidf_eval = evaluate_qa(rag_qa, rag_qa_pairs_tfidf)\n",
        "present_results(rag_tfidf_eval)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-pHZK8bHfUU5",
        "outputId": "03513aba-15b8-4608-8f16-7ebb0f6422bb"
      },
      "id": "-pHZK8bHfUU5",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving contexts: 100%|██████████| 250/250 [00:01<00:00, 139.67it/s]\n",
            "Evaluating QA instances: 100%|██████████| 250/250 [00:56<00:00,  4.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Evaluation Results:\n",
            "Exact Match: 36.40%\n",
            "F1 Score: 46.67%\n",
            "ROUGE2 F1: 23.14%\n",
            "Question: Who is the headmaster of the Christian Brothers of Ireland Stella Maris College?\n",
            "Gold Answer: professor Juan Pedro Toni\n",
            "Predicted Answer: Juan Pedro Toni\n",
            "Exact Match: 0, F1 Score: 0.8571428571428571\n",
            "ROUGE-2 F1-score: 0.8\n",
            "----------------------------------------\n",
            "Question: What is the ratio of black and Asian schoolchildren to white schoolchildren?\n",
            "Gold Answer: about six to four\n",
            "Predicted Answer: 1/2\n",
            "Exact Match: 0, F1 Score: 0.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did Outcault's The Yellow Kid appear in newspapers?\n",
            "Gold Answer: 1890s\n",
            "Predicted Answer: 1890s\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: When did devolution in the UK begin?\n",
            "Gold Answer: 1914\n",
            "Predicted Answer: devolution in the UK began with the Government of Ireland Act 1914\n",
            "Exact Match: 0, F1 Score: 0.18181818181818182\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n",
            "Question: Treating the mitrailleuse like what rendered it far less effective\n",
            "Gold Answer: artillery\n",
            "Predicted Answer: artillery\n",
            "Exact Match: 1, F1 Score: 1.0\n",
            "ROUGE-2 F1-score: 0.0\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bacbf0d5aa41468898c728e8cbfaa0bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_66b7b83e58894830a3bdc8ac77c54413",
              "IPY_MODEL_1aec602be1e142c9a3e408aad2618554",
              "IPY_MODEL_5c9ec76ff16a458bb9ead5c0ce6c3482"
            ],
            "layout": "IPY_MODEL_385bdfb4a228467288b5a53f03956642"
          }
        },
        "66b7b83e58894830a3bdc8ac77c54413": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e015f285f834156b553734d34c0e4d5",
            "placeholder": "​",
            "style": "IPY_MODEL_a346728ded554d5cba9c4e0c803d1b59",
            "value": "config.json: 100%"
          }
        },
        "1aec602be1e142c9a3e408aad2618554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8eb55be214b4373a8b9687852baa5cf",
            "max": 625,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3872cd3aa4b404c87225184d9f5d37f",
            "value": 625
          }
        },
        "5c9ec76ff16a458bb9ead5c0ce6c3482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e0eff1dbe3748a392c001e5261c856b",
            "placeholder": "​",
            "style": "IPY_MODEL_9abd56f1ec654e2cb206dc7c9fcbdaae",
            "value": " 625/625 [00:00&lt;00:00, 78.9kB/s]"
          }
        },
        "385bdfb4a228467288b5a53f03956642": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e015f285f834156b553734d34c0e4d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a346728ded554d5cba9c4e0c803d1b59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8eb55be214b4373a8b9687852baa5cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3872cd3aa4b404c87225184d9f5d37f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e0eff1dbe3748a392c001e5261c856b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9abd56f1ec654e2cb206dc7c9fcbdaae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adbdad65ba814a7f91c1fd6a47158f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bed6e171f40140a7bb23546e10faaa57",
              "IPY_MODEL_f24f128689234e87bd6bee26cd3df6cf",
              "IPY_MODEL_f16b9bfa200342a3ac7bd370a59bbaf2"
            ],
            "layout": "IPY_MODEL_f26778c56f304da09d83cfa68c14471e"
          }
        },
        "bed6e171f40140a7bb23546e10faaa57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49f8af4cb1b14c939f89ee243c99bb10",
            "placeholder": "​",
            "style": "IPY_MODEL_f81dbda99bc142faae3cd32e7078cf7b",
            "value": "model.safetensors: 100%"
          }
        },
        "f24f128689234e87bd6bee26cd3df6cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74908ad895234c048a017fef401ad992",
            "max": 2969854224,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_009a8b6439fc446da361b01de533587b",
            "value": 2969854224
          }
        },
        "f16b9bfa200342a3ac7bd370a59bbaf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33a869015a45439fa5a71265e6b7ee6b",
            "placeholder": "​",
            "style": "IPY_MODEL_bb7d6198a1e54de8a8f10fb48e248c89",
            "value": " 2.97G/2.97G [00:08&lt;00:00, 111MB/s]"
          }
        },
        "f26778c56f304da09d83cfa68c14471e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49f8af4cb1b14c939f89ee243c99bb10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f81dbda99bc142faae3cd32e7078cf7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74908ad895234c048a017fef401ad992": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "009a8b6439fc446da361b01de533587b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "33a869015a45439fa5a71265e6b7ee6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb7d6198a1e54de8a8f10fb48e248c89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cec7a23950634a10ae2770a83a22d4a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_39627779680e45639f63b81295a52434",
              "IPY_MODEL_4bd9a6052bfa413183c8e155b8d62950",
              "IPY_MODEL_473710553d9442edaedf74279b34677e"
            ],
            "layout": "IPY_MODEL_8e9cf6a306a447cc921b1c354e0eb48b"
          }
        },
        "39627779680e45639f63b81295a52434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9f4caf6542640789a282cfcbe553a85",
            "placeholder": "​",
            "style": "IPY_MODEL_2fe5f5a752794780b5f3962be13937c3",
            "value": "generation_config.json: 100%"
          }
        },
        "4bd9a6052bfa413183c8e155b8d62950": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9fa367e15f44ff9957f5541238a282e",
            "max": 121,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8613b9cf566c4b2e95730a19c19adbf2",
            "value": 121
          }
        },
        "473710553d9442edaedf74279b34677e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41ff57fd27e54858b605384861111c7d",
            "placeholder": "​",
            "style": "IPY_MODEL_b14b6e2b9e7d4ef5a635c4bdf7373c9a",
            "value": " 121/121 [00:00&lt;00:00, 15.3kB/s]"
          }
        },
        "8e9cf6a306a447cc921b1c354e0eb48b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9f4caf6542640789a282cfcbe553a85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fe5f5a752794780b5f3962be13937c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9fa367e15f44ff9957f5541238a282e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8613b9cf566c4b2e95730a19c19adbf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41ff57fd27e54858b605384861111c7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b14b6e2b9e7d4ef5a635c4bdf7373c9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8993eb9a4578435d8324be0905ec3815": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_52fd0527d4624155aa60ae36ea969973",
              "IPY_MODEL_40ef0dad267c4adcbc90134c10493dfb",
              "IPY_MODEL_5f08a54f832a41a0942a126d9c6a94c1"
            ],
            "layout": "IPY_MODEL_9645fae93e834d9bb3d557cd6419e613"
          }
        },
        "52fd0527d4624155aa60ae36ea969973": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed5e33ee7a9542eab3e4e5a8496fbc2b",
            "placeholder": "​",
            "style": "IPY_MODEL_ecd9371c0d7d45cea1bcc63e10649564",
            "value": "tokenizer_config.json: "
          }
        },
        "40ef0dad267c4adcbc90134c10493dfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_380c5a9c507746da883de66a8a83227a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9e1d00f5fdf4fa6a86c2ab24a984851",
            "value": 1
          }
        },
        "5f08a54f832a41a0942a126d9c6a94c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2f60010bd334ed7956069230e967fc1",
            "placeholder": "​",
            "style": "IPY_MODEL_4bdfa4a10f7c4db5b1abf1463d3da65f",
            "value": " 4.88k/? [00:00&lt;00:00, 557kB/s]"
          }
        },
        "9645fae93e834d9bb3d557cd6419e613": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed5e33ee7a9542eab3e4e5a8496fbc2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecd9371c0d7d45cea1bcc63e10649564": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "380c5a9c507746da883de66a8a83227a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "c9e1d00f5fdf4fa6a86c2ab24a984851": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2f60010bd334ed7956069230e967fc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bdfa4a10f7c4db5b1abf1463d3da65f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0c8b902dcc3493d8ff32fa96cc1a248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d5fe363e1c854f3ca9163e8e2a095ccc",
              "IPY_MODEL_9fc169ffdd9f4e4c903dbba08f70a724",
              "IPY_MODEL_99500cb2196a4c1dba1cb2b5f3050880"
            ],
            "layout": "IPY_MODEL_acbe2492798845b085f3a1d5599d6759"
          }
        },
        "d5fe363e1c854f3ca9163e8e2a095ccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1992dfd99c64d4dba9dfc5a60f4b8d3",
            "placeholder": "​",
            "style": "IPY_MODEL_b2fb4ff5e2da44efad9c57f7a7e4d376",
            "value": "vocab.json: "
          }
        },
        "9fc169ffdd9f4e4c903dbba08f70a724": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b5fa2a6452c44a696773171249aaab4",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e0038054b81407b81498d50c4c7b60a",
            "value": 1
          }
        },
        "99500cb2196a4c1dba1cb2b5f3050880": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_037746ed4b0448dda0bb7d0f830bb834",
            "placeholder": "​",
            "style": "IPY_MODEL_492f6067409e4cd3ab250a1bc8316d30",
            "value": " 1.61M/? [00:00&lt;00:00, 69.0MB/s]"
          }
        },
        "acbe2492798845b085f3a1d5599d6759": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1992dfd99c64d4dba9dfc5a60f4b8d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2fb4ff5e2da44efad9c57f7a7e4d376": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b5fa2a6452c44a696773171249aaab4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8e0038054b81407b81498d50c4c7b60a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "037746ed4b0448dda0bb7d0f830bb834": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "492f6067409e4cd3ab250a1bc8316d30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eaad9a77e5a7406694b03d4b7c987263": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a1710b475294e2590438ce7c47e57d1",
              "IPY_MODEL_9d68823b026d4f1280981b21160d3996",
              "IPY_MODEL_39e3299e26c144c1abdc4ee10017a1b0"
            ],
            "layout": "IPY_MODEL_83afe6ab85854dfb992651044a1049ae"
          }
        },
        "7a1710b475294e2590438ce7c47e57d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_583a2c447a184a7ead01b5e0d6f421ad",
            "placeholder": "​",
            "style": "IPY_MODEL_704fa2ed86774180b02ab9eab2d85f47",
            "value": "merges.txt: "
          }
        },
        "9d68823b026d4f1280981b21160d3996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4be93ba8b7874aabbc517037ed838d28",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bf0d772dd1f6415cbca20c08fb87e70a",
            "value": 1
          }
        },
        "39e3299e26c144c1abdc4ee10017a1b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c84e47bcb3347e98350d6be60e0dc69",
            "placeholder": "​",
            "style": "IPY_MODEL_3013ae45bab34f45b453c17e3efaeea5",
            "value": " 917k/? [00:00&lt;00:00, 54.6MB/s]"
          }
        },
        "83afe6ab85854dfb992651044a1049ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "583a2c447a184a7ead01b5e0d6f421ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "704fa2ed86774180b02ab9eab2d85f47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4be93ba8b7874aabbc517037ed838d28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "bf0d772dd1f6415cbca20c08fb87e70a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c84e47bcb3347e98350d6be60e0dc69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3013ae45bab34f45b453c17e3efaeea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4dc29f35310447efb710946feaa5aca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed9036e0fc554e19bfa1f00e2149d71c",
              "IPY_MODEL_1885b4f7cf764685a00945b0a29c62c0",
              "IPY_MODEL_cc3cdd5cd14b4fa99c2651d4573351d4"
            ],
            "layout": "IPY_MODEL_8e54992a77fd483eb387a587e5318cbe"
          }
        },
        "ed9036e0fc554e19bfa1f00e2149d71c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49e0eecead9846f79369c527ca7e0938",
            "placeholder": "​",
            "style": "IPY_MODEL_a813b8a372d34d3a84c954ee5bc6dfef",
            "value": "tokenizer.json: "
          }
        },
        "1885b4f7cf764685a00945b0a29c62c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25c9e65639d145f382a77823a706dee2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e03e761b6f994fc6be3d0bb7ee31234d",
            "value": 1
          }
        },
        "cc3cdd5cd14b4fa99c2651d4573351d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c5ecf66f1214125b6b87584e48878fb",
            "placeholder": "​",
            "style": "IPY_MODEL_784237dbc0f44096a17665780f3affd4",
            "value": " 7.14M/? [00:00&lt;00:00, 168MB/s]"
          }
        },
        "8e54992a77fd483eb387a587e5318cbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49e0eecead9846f79369c527ca7e0938": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a813b8a372d34d3a84c954ee5bc6dfef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25c9e65639d145f382a77823a706dee2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e03e761b6f994fc6be3d0bb7ee31234d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c5ecf66f1214125b6b87584e48878fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "784237dbc0f44096a17665780f3affd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5f5c238b31a544abbbc66e7db43a7996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64ec545d84924e6aa67bcb460d9084f1",
              "IPY_MODEL_eafeb3762b8e41a5bd958f1ff25c8090",
              "IPY_MODEL_5b8f39543cea430e8b2135d1a061616b"
            ],
            "layout": "IPY_MODEL_22c30e860b034497890b31e97077f550"
          }
        },
        "64ec545d84924e6aa67bcb460d9084f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_801fe698b3244e0987da734b60a8038a",
            "placeholder": "​",
            "style": "IPY_MODEL_d69ec1575da441899a58e917b5c55d9c",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "eafeb3762b8e41a5bd958f1ff25c8090": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abc520958046430a8c439fe93f674fa9",
            "max": 581,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2faf73e4aeb74568bdad860d6d1824e0",
            "value": 581
          }
        },
        "5b8f39543cea430e8b2135d1a061616b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3289ed4a189248e292d39371b91cbfb4",
            "placeholder": "​",
            "style": "IPY_MODEL_6289b4af693c42d7980cced4a770ffda",
            "value": " 581/581 [00:00&lt;00:00, 78.6kB/s]"
          }
        },
        "22c30e860b034497890b31e97077f550": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "801fe698b3244e0987da734b60a8038a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d69ec1575da441899a58e917b5c55d9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abc520958046430a8c439fe93f674fa9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2faf73e4aeb74568bdad860d6d1824e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3289ed4a189248e292d39371b91cbfb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6289b4af693c42d7980cced4a770ffda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}